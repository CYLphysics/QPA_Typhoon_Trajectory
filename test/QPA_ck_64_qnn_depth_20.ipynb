{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import random \n",
    "import qadence as qd\n",
    "import numpy as np \n",
    "import time \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "# from torchsummary import summary\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.dirname(current_dir))\n",
    "\n",
    "from code_base.qpa_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing for the Typhoon dataset\n",
    "\n",
    "Before diving into the exciting Quantum Parameter Adaptation section, we first need to preprocess the data. Since data processing is not the main focus of this project, we will only provide a brief overview. Interested readers can refer to the original paper ([here](https://www.researchgate.net/publication/357911189_AM-ConvGRU_a_spatio-temporal_model_for_typhoon_path_prediction)) for a detailed explanation and the full classical machine learning model.\n",
    "\n",
    "![](https://github.com/CYLphysics/QPA_Typhoon_Trajectory/blob/main/test/figure/total_data.png)\n"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# forecast 24-hour lead time \n",
    "pre_seq = 4\n",
    "batch_size = 128\n",
    "epochs = 256\n",
    "min_val_loss = 100\n",
    "model_name = '../results/model_saver/QPA_ck_64_qnn_depth_20.pkl'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 1 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 2 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 3 (8406, 1, 4, 31, 31)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "test = pd.read_csv('../data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "\n",
    "\n",
    "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
    "CLIPER_feature.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "X_wide_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 6:])\n",
    "X_wide_train = X_wide[0: train.shape[0], :]\n",
    "\n",
    "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3:4])\n",
    "y_train = y[0: train.shape[0], :]\n",
    "\n",
    "\n",
    "reanalysis_type = 'z'\n",
    "# 0 means now \n",
    "# 1 means 6-hour ago\n",
    "# 2 means 12-hour ago\n",
    "ahead_times = [0,1,2,3]\n",
    "pressures = [1000, 750, 500, 250]\n",
    "sequential_reanalysis_list = []\n",
    "reanalysis_test_dict = {}\n",
    "X_deep_scaler_dict = {}\n",
    "\n",
    "\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        \n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "        train_reanalysis_csv = pd.read_csv('../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "        \n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis\n",
    "        \n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        X_deep = X_deep_scaler_dict[scaler_name] .fit_transform(reanalysis.loc[:, 5:])\n",
    "        \n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "    \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_train = np.concatenate(sequential_reanalysis_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of training set and validation set\n",
    "\n",
    "The typhoon data from 2000 to 2014 is used for training, while data from 2015 to 2018 is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoader(Data.Dataset):\n",
    "    def __init__(self, X_wide_train, X_deep_train, y_train):\n",
    "        self.X_wide_train = X_wide_train\n",
    "        self.X_deep_train = X_deep_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return [self.X_wide_train[index], self.X_deep_train[index]], self.y_train[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_wide_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Parameter Adaptation (QPA)\n",
    "\n",
    "The implementation of Quantum Parameter Adaptation (QPA) is provided in `code_base/qpa_utils.py`. \n",
    "![](figure/qeel.png)\n",
    "\n",
    "<!-- Interestingly, the core concept of the QPA is that, we compress only the training parameters during training, using QML technique (more detail in the technical report). Thus, during the inference stage, ie.. the daily usage of the trained model, do not require the usage of quantum computer, since the training result of QPA is a pure classical model. QPA is a Quantum-Train-based (QT-based) method, the comparison of the computational scheme for conventional QML and QT is shown below. Furthermore, as one may observe, the QT-based method don't require the data encoding part of the quantum circuit, since the data is inputted directly into the classical model, thus QT (QPA) also eliminate the issue of data encoding, which is very challenging when the input data is large. Lastly, QT (QPA) utlize the power of Hilbert space, assuming the polynomial number of QNN layers are used, training the classical model with $m$ parameters only requires $polylog(m)$ parameters. (more detail in the section 2 of the technical report.) -->\n",
    " \n",
    "The core concept of QPA is that we compress only the training parameters during training using Quantum Machine Learning (QML) techniques (see the technical report for more details). During inference—i.e., the daily usage of the trained model—no quantum computing resources are required, as the output of QPA is a purely classical model.\n",
    "\n",
    "QPA is a Quantum-Train-based (QT-based) method, and a comparison of the computational schemes between conventional QML and QT is shown below. Notably, QT-based methods do not require a quantum data encoding process, since the data is directly input into the classical model. This eliminates the data encoding challenge, which becomes increasingly difficult as the input data size grows.\n",
    "\n",
    "Furthermore, QT (QPA) leverages the power of Hilbert space. Assuming a polynomial number of Quantum Neural Network (QNN) layers are used, training a classical model with $ m $ parameters only requires $ polylog(m)$  parameters (see Section 2 of the technical report for further details).\n",
    "\n",
    "![](figure/qml_and_qt.png)\n",
    "\n",
    "<!-- Thus, in the below notebook block, although the original ML model have about 8399540 trainable parameters, one can see in our QPA only require 216398 parameters (under specific hyperparameter setting). This is only 2.57% of the original model. And the qubit usage is 8, as shown below. The calculation of the qubit usage can be found in the section 2 of the technical report. \n",
    " -->\n",
    "\n",
    "In the notebook block below, while the original machine learning model has 8,399,540 trainable parameters, our QPA approach—under a specific hyperparameter setting—requires only 216,398 parameters. This is merely 2.57% of the original model’s size, demonstrating significant parameter reduction. Additionally, the qubit usage is only 8, as shown below. The detailed calculation of qubit usage can be found in Section 2 of the technical report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyu/anaconda3/envs/qttp/lib/python3.9/site-packages/torch/nn/modules/module.py:1313: UserWarning: Complex modules are a new feature under active development whose design may change, and some modules might not work as expected when using complex tensors as parameters or buffers. Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml if a complex module does not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameters in the current model:  216398\n",
      " # of required qubits in the current QPA setting:  8\n"
     ]
    }
   ],
   "source": [
    "net = Net() \n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "total_steps = len(train_dataset) * epochs  \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "print(\"# of trainable parameters in the current model: \",\n",
    "    sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    ")\n",
    "\n",
    "print(\" # of required qubits in the current QPA setting: \", \n",
    "      net.fc1.grand_hypernetwork[0].n_qubit\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyu/QPA_Typhoon_Trajectory/code_base/qpa_utils.py:442: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:308.)\n",
      "  x = x.to(torch.float32).cuda()\n",
      "/home/chenyu/anaconda3/envs/qttp/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [1/256], cost:50.00s train_loss: 12.02905 val_loss: 1.04195\n",
      "epochs [2/256], cost:53.00s train_loss: 9.00621 val_loss: 0.89334\n",
      "epochs [3/256], cost:54.00s train_loss: 7.00400 val_loss: 0.82309\n",
      "epochs [4/256], cost:53.00s train_loss: 6.05885 val_loss: 0.65579\n",
      "epochs [5/256], cost:54.00s train_loss: 5.05560 val_loss: 0.59409\n",
      "epochs [6/256], cost:53.00s train_loss: 4.48057 val_loss: 0.48360\n",
      "epochs [7/256], cost:53.00s train_loss: 4.19932 val_loss: 0.47856\n",
      "epochs [8/256], cost:54.00s train_loss: 4.08354 val_loss: 0.44939\n",
      "epochs [9/256], cost:53.00s train_loss: 4.11250 val_loss: 0.46782\n",
      "epochs [10/256], cost:53.00s train_loss: 4.01919 val_loss: 0.48235\n",
      "epochs [11/256], cost:52.00s train_loss: 3.88076 val_loss: 0.46335\n",
      "epochs [12/256], cost:54.00s train_loss: 3.81757 val_loss: 0.42281\n",
      "epochs [13/256], cost:54.00s train_loss: 3.76393 val_loss: 0.43794\n",
      "epochs [14/256], cost:53.00s train_loss: 3.83981 val_loss: 0.48410\n",
      "epochs [15/256], cost:53.00s train_loss: 3.69659 val_loss: 0.39719\n",
      "epochs [16/256], cost:53.00s train_loss: 3.64808 val_loss: 0.46725\n",
      "epochs [17/256], cost:53.00s train_loss: 3.58511 val_loss: 0.40232\n",
      "epochs [18/256], cost:53.00s train_loss: 3.49645 val_loss: 0.44956\n",
      "epochs [19/256], cost:53.00s train_loss: 3.51578 val_loss: 0.40427\n",
      "epochs [20/256], cost:53.00s train_loss: 3.43931 val_loss: 0.41692\n",
      "epochs [21/256], cost:54.00s train_loss: 3.37977 val_loss: 0.42085\n",
      "epochs [22/256], cost:54.00s train_loss: 3.44391 val_loss: 0.37470\n",
      "epochs [23/256], cost:54.00s train_loss: 3.18838 val_loss: 0.34748\n",
      "epochs [24/256], cost:53.00s train_loss: 3.11685 val_loss: 0.33329\n",
      "epochs [25/256], cost:54.00s train_loss: 2.79813 val_loss: 0.41017\n",
      "epochs [26/256], cost:52.00s train_loss: 2.84746 val_loss: 0.36443\n",
      "epochs [27/256], cost:53.00s train_loss: 2.88251 val_loss: 0.33819\n",
      "epochs [28/256], cost:53.00s train_loss: 2.60309 val_loss: 0.29963\n",
      "epochs [29/256], cost:54.00s train_loss: 2.37074 val_loss: 0.24064\n",
      "epochs [30/256], cost:53.00s train_loss: 2.05729 val_loss: 0.25250\n",
      "epochs [31/256], cost:56.00s train_loss: 2.20697 val_loss: 0.22889\n",
      "epochs [32/256], cost:57.00s train_loss: 2.08384 val_loss: 0.23204\n",
      "epochs [33/256], cost:56.00s train_loss: 2.04607 val_loss: 0.21140\n",
      "epochs [34/256], cost:56.00s train_loss: 2.04673 val_loss: 0.21334\n",
      "epochs [35/256], cost:57.00s train_loss: 1.87249 val_loss: 0.19930\n",
      "epochs [36/256], cost:59.00s train_loss: 1.75682 val_loss: 0.17941\n",
      "epochs [37/256], cost:57.00s train_loss: 1.66541 val_loss: 0.22062\n",
      "epochs [38/256], cost:58.00s train_loss: 1.69878 val_loss: 0.19342\n",
      "epochs [39/256], cost:57.00s train_loss: 1.73732 val_loss: 0.16924\n",
      "epochs [40/256], cost:57.00s train_loss: 1.64750 val_loss: 0.20937\n",
      "epochs [41/256], cost:58.00s train_loss: 1.65359 val_loss: 0.19154\n",
      "epochs [42/256], cost:59.00s train_loss: 1.45113 val_loss: 0.15946\n",
      "epochs [43/256], cost:58.00s train_loss: 1.60170 val_loss: 0.18069\n",
      "epochs [44/256], cost:58.00s train_loss: 1.66916 val_loss: 0.19867\n",
      "epochs [45/256], cost:59.00s train_loss: 1.43576 val_loss: 0.15362\n",
      "epochs [46/256], cost:56.00s train_loss: 1.41797 val_loss: 0.22105\n",
      "epochs [47/256], cost:53.00s train_loss: 1.58330 val_loss: 0.17276\n",
      "epochs [48/256], cost:53.00s train_loss: 1.53164 val_loss: 0.18558\n",
      "epochs [49/256], cost:53.00s train_loss: 1.47104 val_loss: 0.16112\n",
      "epochs [50/256], cost:53.00s train_loss: 1.27258 val_loss: 0.14126\n",
      "epochs [51/256], cost:53.00s train_loss: 1.25844 val_loss: 0.13606\n",
      "epochs [52/256], cost:53.00s train_loss: 1.23851 val_loss: 0.12981\n",
      "epochs [53/256], cost:53.00s train_loss: 1.15721 val_loss: 0.15647\n",
      "epochs [54/256], cost:53.00s train_loss: 1.27383 val_loss: 0.13202\n",
      "epochs [55/256], cost:53.00s train_loss: 1.18140 val_loss: 0.13577\n",
      "epochs [56/256], cost:55.00s train_loss: 1.14995 val_loss: 0.13227\n",
      "epochs [57/256], cost:59.00s train_loss: 1.11425 val_loss: 0.13404\n",
      "epochs [58/256], cost:60.00s train_loss: 1.12872 val_loss: 0.12341\n",
      "epochs [59/256], cost:56.00s train_loss: 1.19568 val_loss: 0.13530\n",
      "epochs [60/256], cost:54.00s train_loss: 1.07207 val_loss: 0.14374\n",
      "epochs [61/256], cost:54.00s train_loss: 1.10006 val_loss: 0.12221\n",
      "epochs [62/256], cost:54.00s train_loss: 1.10637 val_loss: 0.15190\n",
      "epochs [63/256], cost:54.00s train_loss: 1.14089 val_loss: 0.13129\n",
      "epochs [64/256], cost:53.00s train_loss: 1.30407 val_loss: 0.12885\n",
      "epochs [65/256], cost:54.00s train_loss: 1.15776 val_loss: 0.13853\n",
      "epochs [66/256], cost:56.00s train_loss: 1.17526 val_loss: 0.14589\n",
      "epochs [67/256], cost:54.00s train_loss: 1.26472 val_loss: 0.13179\n",
      "epochs [68/256], cost:55.00s train_loss: 1.06305 val_loss: 0.10672\n",
      "epochs [69/256], cost:54.00s train_loss: 0.99230 val_loss: 0.11312\n",
      "epochs [70/256], cost:54.00s train_loss: 0.98806 val_loss: 0.12650\n",
      "epochs [71/256], cost:53.00s train_loss: 1.06712 val_loss: 0.12434\n",
      "epochs [72/256], cost:54.00s train_loss: 1.00725 val_loss: 0.12394\n",
      "epochs [73/256], cost:54.00s train_loss: 1.12122 val_loss: 0.15796\n",
      "epochs [74/256], cost:54.00s train_loss: 1.10213 val_loss: 0.13924\n",
      "epochs [75/256], cost:54.00s train_loss: 1.02494 val_loss: 0.10969\n",
      "epochs [76/256], cost:55.00s train_loss: 0.92992 val_loss: 0.10100\n",
      "epochs [77/256], cost:54.00s train_loss: 0.91588 val_loss: 0.10621\n",
      "epochs [78/256], cost:53.00s train_loss: 0.95468 val_loss: 0.09468\n",
      "epochs [79/256], cost:53.00s train_loss: 0.89877 val_loss: 0.10912\n",
      "epochs [80/256], cost:53.00s train_loss: 0.91082 val_loss: 0.10747\n",
      "epochs [81/256], cost:53.00s train_loss: 0.96070 val_loss: 0.11684\n",
      "epochs [82/256], cost:53.00s train_loss: 0.96552 val_loss: 0.10834\n",
      "epochs [83/256], cost:53.00s train_loss: 0.97759 val_loss: 0.10057\n",
      "epochs [84/256], cost:53.00s train_loss: 0.91768 val_loss: 0.09633\n",
      "epochs [85/256], cost:54.00s train_loss: 0.96483 val_loss: 0.09884\n",
      "epochs [86/256], cost:54.00s train_loss: 0.90666 val_loss: 0.10401\n",
      "epochs [87/256], cost:54.00s train_loss: 0.84665 val_loss: 0.11000\n",
      "epochs [88/256], cost:55.00s train_loss: 0.86986 val_loss: 0.09422\n",
      "epochs [89/256], cost:54.00s train_loss: 0.82724 val_loss: 0.10597\n",
      "epochs [90/256], cost:55.00s train_loss: 0.80930 val_loss: 0.09323\n",
      "epochs [91/256], cost:54.00s train_loss: 0.80143 val_loss: 0.10480\n",
      "epochs [92/256], cost:53.00s train_loss: 0.93466 val_loss: 0.11235\n",
      "epochs [93/256], cost:52.00s train_loss: 0.90478 val_loss: 0.10503\n",
      "epochs [94/256], cost:50.00s train_loss: 0.81868 val_loss: 0.09353\n",
      "epochs [95/256], cost:50.00s train_loss: 0.84483 val_loss: 0.09892\n",
      "epochs [96/256], cost:50.00s train_loss: 0.90500 val_loss: 0.10604\n",
      "epochs [97/256], cost:50.00s train_loss: 0.83944 val_loss: 0.09824\n",
      "epochs [98/256], cost:51.00s train_loss: 0.77762 val_loss: 0.08512\n",
      "epochs [99/256], cost:50.00s train_loss: 0.79158 val_loss: 0.09225\n",
      "epochs [100/256], cost:56.00s train_loss: 0.71069 val_loss: 0.08465\n",
      "epochs [101/256], cost:53.00s train_loss: 0.70474 val_loss: 0.08284\n",
      "epochs [102/256], cost:50.00s train_loss: 0.75492 val_loss: 0.08927\n",
      "epochs [103/256], cost:53.00s train_loss: 0.78995 val_loss: 0.09771\n",
      "epochs [104/256], cost:53.00s train_loss: 0.71278 val_loss: 0.07949\n",
      "epochs [105/256], cost:53.00s train_loss: 0.68508 val_loss: 0.07868\n",
      "epochs [106/256], cost:53.00s train_loss: 0.71150 val_loss: 0.09431\n",
      "epochs [107/256], cost:52.00s train_loss: 0.73961 val_loss: 0.08391\n",
      "epochs [108/256], cost:54.00s train_loss: 0.77887 val_loss: 0.08112\n",
      "epochs [109/256], cost:53.00s train_loss: 0.72273 val_loss: 0.08788\n",
      "epochs [110/256], cost:54.00s train_loss: 0.70649 val_loss: 0.07696\n",
      "epochs [111/256], cost:53.00s train_loss: 0.69967 val_loss: 0.07606\n",
      "epochs [112/256], cost:53.00s train_loss: 0.76074 val_loss: 0.09292\n",
      "epochs [113/256], cost:53.00s train_loss: 0.74478 val_loss: 0.08182\n",
      "epochs [114/256], cost:52.00s train_loss: 0.74672 val_loss: 0.07693\n",
      "epochs [115/256], cost:53.00s train_loss: 0.66084 val_loss: 0.07485\n",
      "epochs [116/256], cost:54.00s train_loss: 0.67851 val_loss: 0.07875\n",
      "epochs [117/256], cost:53.00s train_loss: 0.62964 val_loss: 0.07439\n",
      "epochs [118/256], cost:53.00s train_loss: 0.64152 val_loss: 0.08226\n",
      "epochs [119/256], cost:53.00s train_loss: 0.65465 val_loss: 0.07778\n",
      "epochs [120/256], cost:53.00s train_loss: 0.67054 val_loss: 0.07540\n",
      "epochs [121/256], cost:53.00s train_loss: 0.64949 val_loss: 0.07239\n",
      "epochs [122/256], cost:53.00s train_loss: 0.66413 val_loss: 0.08481\n",
      "epochs [123/256], cost:54.00s train_loss: 0.64169 val_loss: 0.07301\n",
      "epochs [124/256], cost:53.00s train_loss: 0.59903 val_loss: 0.07040\n",
      "epochs [125/256], cost:53.00s train_loss: 0.61950 val_loss: 0.06800\n",
      "epochs [126/256], cost:53.00s train_loss: 0.62071 val_loss: 0.06841\n",
      "epochs [127/256], cost:54.00s train_loss: 0.62776 val_loss: 0.06886\n",
      "epochs [128/256], cost:53.00s train_loss: 0.65265 val_loss: 0.07102\n",
      "epochs [129/256], cost:53.00s train_loss: 0.64347 val_loss: 0.07505\n",
      "epochs [130/256], cost:53.00s train_loss: 0.68706 val_loss: 0.07486\n",
      "epochs [131/256], cost:54.00s train_loss: 0.64385 val_loss: 0.07341\n",
      "epochs [132/256], cost:53.00s train_loss: 0.58123 val_loss: 0.06487\n",
      "epochs [133/256], cost:53.00s train_loss: 0.57352 val_loss: 0.06535\n",
      "epochs [134/256], cost:53.00s train_loss: 0.56570 val_loss: 0.06472\n",
      "epochs [135/256], cost:53.00s train_loss: 0.61002 val_loss: 0.06976\n",
      "epochs [136/256], cost:53.00s train_loss: 0.64470 val_loss: 0.06842\n",
      "epochs [137/256], cost:53.00s train_loss: 0.62432 val_loss: 0.07704\n",
      "epochs [138/256], cost:53.00s train_loss: 0.62631 val_loss: 0.06786\n",
      "epochs [139/256], cost:53.00s train_loss: 0.59710 val_loss: 0.06796\n",
      "epochs [140/256], cost:53.00s train_loss: 0.59595 val_loss: 0.06803\n",
      "epochs [141/256], cost:53.00s train_loss: 0.56730 val_loss: 0.06758\n",
      "epochs [142/256], cost:53.00s train_loss: 0.55865 val_loss: 0.06150\n",
      "epochs [143/256], cost:53.00s train_loss: 0.55418 val_loss: 0.06470\n",
      "epochs [144/256], cost:53.00s train_loss: 0.64967 val_loss: 0.06889\n",
      "epochs [145/256], cost:54.00s train_loss: 0.58854 val_loss: 0.06987\n",
      "epochs [146/256], cost:53.00s train_loss: 0.58539 val_loss: 0.05978\n",
      "epochs [147/256], cost:53.00s train_loss: 0.54723 val_loss: 0.07422\n",
      "epochs [148/256], cost:52.00s train_loss: 0.58883 val_loss: 0.06433\n",
      "epochs [149/256], cost:53.00s train_loss: 0.55110 val_loss: 0.05965\n",
      "epochs [150/256], cost:53.00s train_loss: 0.54401 val_loss: 0.06127\n",
      "epochs [151/256], cost:52.00s train_loss: 0.58581 val_loss: 0.06596\n",
      "epochs [152/256], cost:53.00s train_loss: 0.58458 val_loss: 0.06683\n",
      "epochs [153/256], cost:53.00s train_loss: 0.55653 val_loss: 0.06805\n",
      "epochs [154/256], cost:54.00s train_loss: 0.53567 val_loss: 0.05838\n",
      "epochs [155/256], cost:53.00s train_loss: 0.49267 val_loss: 0.06024\n",
      "epochs [156/256], cost:53.00s train_loss: 0.49733 val_loss: 0.06077\n",
      "epochs [157/256], cost:53.00s train_loss: 0.50405 val_loss: 0.05754\n",
      "epochs [158/256], cost:52.00s train_loss: 0.48821 val_loss: 0.05723\n",
      "epochs [159/256], cost:53.00s train_loss: 0.49908 val_loss: 0.06047\n",
      "epochs [160/256], cost:52.00s train_loss: 0.50861 val_loss: 0.05831\n",
      "epochs [161/256], cost:50.00s train_loss: 0.48792 val_loss: 0.05726\n",
      "epochs [162/256], cost:50.00s train_loss: 0.49626 val_loss: 0.05702\n",
      "epochs [163/256], cost:50.00s train_loss: 0.48548 val_loss: 0.06241\n",
      "epochs [164/256], cost:50.00s train_loss: 0.51082 val_loss: 0.05870\n",
      "epochs [165/256], cost:50.00s train_loss: 0.48561 val_loss: 0.05766\n",
      "epochs [166/256], cost:54.00s train_loss: 0.50930 val_loss: 0.05628\n",
      "epochs [167/256], cost:54.00s train_loss: 0.48358 val_loss: 0.05746\n",
      "epochs [168/256], cost:53.00s train_loss: 0.47651 val_loss: 0.05869\n",
      "epochs [169/256], cost:54.00s train_loss: 0.50174 val_loss: 0.05843\n",
      "epochs [170/256], cost:53.00s train_loss: 0.48661 val_loss: 0.05372\n",
      "epochs [171/256], cost:53.00s train_loss: 0.46099 val_loss: 0.06266\n",
      "epochs [172/256], cost:53.00s train_loss: 0.47895 val_loss: 0.06459\n",
      "epochs [173/256], cost:52.00s train_loss: 0.48572 val_loss: 0.05983\n",
      "epochs [174/256], cost:53.00s train_loss: 0.47310 val_loss: 0.05498\n",
      "epochs [175/256], cost:53.00s train_loss: 0.46160 val_loss: 0.05279\n",
      "epochs [176/256], cost:53.00s train_loss: 0.44904 val_loss: 0.05188\n",
      "epochs [177/256], cost:54.00s train_loss: 0.44208 val_loss: 0.05030\n",
      "epochs [178/256], cost:53.00s train_loss: 0.44177 val_loss: 0.05132\n",
      "epochs [179/256], cost:53.00s train_loss: 0.45509 val_loss: 0.05294\n",
      "epochs [180/256], cost:53.00s train_loss: 0.43962 val_loss: 0.05018\n",
      "epochs [181/256], cost:53.00s train_loss: 0.43061 val_loss: 0.04994\n",
      "epochs [182/256], cost:53.00s train_loss: 0.43516 val_loss: 0.05640\n",
      "epochs [183/256], cost:54.00s train_loss: 0.46113 val_loss: 0.05301\n",
      "epochs [184/256], cost:53.00s train_loss: 0.43247 val_loss: 0.04764\n",
      "epochs [185/256], cost:52.00s train_loss: 0.43195 val_loss: 0.04998\n",
      "epochs [186/256], cost:53.00s train_loss: 0.45414 val_loss: 0.05182\n",
      "epochs [187/256], cost:52.00s train_loss: 0.41986 val_loss: 0.04962\n",
      "epochs [188/256], cost:52.00s train_loss: 0.42240 val_loss: 0.04915\n",
      "epochs [189/256], cost:53.00s train_loss: 0.41473 val_loss: 0.05010\n",
      "epochs [190/256], cost:53.00s train_loss: 0.41416 val_loss: 0.04726\n",
      "epochs [191/256], cost:53.00s train_loss: 0.41624 val_loss: 0.05445\n",
      "epochs [192/256], cost:53.00s train_loss: 0.44316 val_loss: 0.05104\n",
      "epochs [193/256], cost:54.00s train_loss: 0.44041 val_loss: 0.05117\n",
      "epochs [194/256], cost:53.00s train_loss: 0.43185 val_loss: 0.04854\n",
      "epochs [195/256], cost:53.00s train_loss: 0.42206 val_loss: 0.05498\n",
      "epochs [196/256], cost:53.00s train_loss: 0.43839 val_loss: 0.04900\n",
      "epochs [197/256], cost:53.00s train_loss: 0.41489 val_loss: 0.04612\n",
      "epochs [198/256], cost:53.00s train_loss: 0.40558 val_loss: 0.04695\n",
      "epochs [199/256], cost:54.00s train_loss: 0.40442 val_loss: 0.04533\n",
      "epochs [200/256], cost:53.00s train_loss: 0.39529 val_loss: 0.04656\n",
      "epochs [201/256], cost:53.00s train_loss: 0.40727 val_loss: 0.04870\n",
      "epochs [202/256], cost:52.00s train_loss: 0.41801 val_loss: 0.04483\n",
      "epochs [203/256], cost:54.00s train_loss: 0.40252 val_loss: 0.04667\n",
      "epochs [204/256], cost:54.00s train_loss: 0.38673 val_loss: 0.04566\n",
      "epochs [205/256], cost:54.00s train_loss: 0.39465 val_loss: 0.04558\n",
      "epochs [206/256], cost:53.00s train_loss: 0.39572 val_loss: 0.04737\n",
      "epochs [207/256], cost:53.00s train_loss: 0.38441 val_loss: 0.04433\n",
      "epochs [208/256], cost:54.00s train_loss: 0.39412 val_loss: 0.04149\n",
      "epochs [209/256], cost:53.00s train_loss: 0.37713 val_loss: 0.04499\n",
      "epochs [210/256], cost:52.00s train_loss: 0.39081 val_loss: 0.04881\n",
      "epochs [211/256], cost:54.00s train_loss: 0.38742 val_loss: 0.04706\n",
      "epochs [212/256], cost:50.00s train_loss: 0.38760 val_loss: 0.04505\n",
      "epochs [213/256], cost:50.00s train_loss: 0.38078 val_loss: 0.04606\n",
      "epochs [214/256], cost:50.00s train_loss: 0.37153 val_loss: 0.04386\n",
      "epochs [215/256], cost:50.00s train_loss: 0.36688 val_loss: 0.04228\n",
      "epochs [216/256], cost:50.00s train_loss: 0.36316 val_loss: 0.04184\n",
      "epochs [217/256], cost:50.00s train_loss: 0.36114 val_loss: 0.04406\n",
      "epochs [218/256], cost:50.00s train_loss: 0.37399 val_loss: 0.04284\n",
      "epochs [219/256], cost:51.00s train_loss: 0.36139 val_loss: 0.04187\n",
      "epochs [220/256], cost:50.00s train_loss: 0.37149 val_loss: 0.04318\n",
      "epochs [221/256], cost:50.00s train_loss: 0.36026 val_loss: 0.05097\n",
      "epochs [222/256], cost:51.00s train_loss: 0.37447 val_loss: 0.04081\n",
      "epochs [223/256], cost:50.00s train_loss: 0.35544 val_loss: 0.04262\n",
      "epochs [224/256], cost:50.00s train_loss: 0.36821 val_loss: 0.04195\n",
      "epochs [225/256], cost:50.00s train_loss: 0.35757 val_loss: 0.04122\n",
      "epochs [226/256], cost:50.00s train_loss: 0.34920 val_loss: 0.04280\n",
      "epochs [227/256], cost:50.00s train_loss: 0.35792 val_loss: 0.03922\n",
      "epochs [228/256], cost:50.00s train_loss: 0.35025 val_loss: 0.04079\n",
      "epochs [229/256], cost:50.00s train_loss: 0.34781 val_loss: 0.04258\n",
      "epochs [230/256], cost:50.00s train_loss: 0.34780 val_loss: 0.03964\n",
      "epochs [231/256], cost:50.00s train_loss: 0.34079 val_loss: 0.04057\n",
      "epochs [232/256], cost:50.00s train_loss: 0.34203 val_loss: 0.04120\n",
      "epochs [233/256], cost:50.00s train_loss: 0.34012 val_loss: 0.03969\n",
      "epochs [234/256], cost:50.00s train_loss: 0.33895 val_loss: 0.03966\n",
      "epochs [235/256], cost:50.00s train_loss: 0.33811 val_loss: 0.04136\n",
      "epochs [236/256], cost:50.00s train_loss: 0.33995 val_loss: 0.04030\n",
      "epochs [237/256], cost:50.00s train_loss: 0.33862 val_loss: 0.03862\n",
      "epochs [238/256], cost:50.00s train_loss: 0.33461 val_loss: 0.03910\n",
      "epochs [239/256], cost:50.00s train_loss: 0.33665 val_loss: 0.03845\n",
      "epochs [240/256], cost:50.00s train_loss: 0.33439 val_loss: 0.03862\n",
      "epochs [241/256], cost:50.00s train_loss: 0.33282 val_loss: 0.03915\n",
      "epochs [242/256], cost:50.00s train_loss: 0.33012 val_loss: 0.03842\n",
      "epochs [243/256], cost:51.00s train_loss: 0.32919 val_loss: 0.03800\n",
      "epochs [244/256], cost:50.00s train_loss: 0.32504 val_loss: 0.03934\n",
      "epochs [245/256], cost:50.00s train_loss: 0.32429 val_loss: 0.03772\n",
      "epochs [246/256], cost:50.00s train_loss: 0.32442 val_loss: 0.03741\n",
      "epochs [247/256], cost:50.00s train_loss: 0.32331 val_loss: 0.04005\n",
      "epochs [248/256], cost:50.00s train_loss: 0.32157 val_loss: 0.03862\n",
      "epochs [249/256], cost:50.00s train_loss: 0.32073 val_loss: 0.03729\n",
      "epochs [250/256], cost:50.00s train_loss: 0.32049 val_loss: 0.03763\n",
      "epochs [251/256], cost:50.00s train_loss: 0.32018 val_loss: 0.03636\n",
      "epochs [252/256], cost:50.00s train_loss: 0.31558 val_loss: 0.04045\n",
      "epochs [253/256], cost:50.00s train_loss: 0.31758 val_loss: 0.03708\n",
      "epochs [254/256], cost:50.00s train_loss: 0.31771 val_loss: 0.03622\n",
      "epochs [255/256], cost:50.00s train_loss: 0.31624 val_loss: 0.03502\n",
      "epochs [256/256], cost:50.00s train_loss: 0.31627 val_loss: 0.03667\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "\n",
    "total_train_loss_list = [] \n",
    "total_val_loss_list = [] \n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    starttime = datetime.datetime.now()\n",
    "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "    train_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    val_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    # training\n",
    "    total_train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        since_batch = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "            # X_wide_train_cuda = batch_x[0].float().cuda()\n",
    "            # X_deep_train_cuda = batch_x[1].float().cuda()\n",
    "            X_wide_train_cuda = batch_x[0].to(torch.complex128).cuda()\n",
    "            X_deep_train_cuda = batch_x[1].to(torch.complex128).cuda()\n",
    "                \n",
    "            y_train_cuda = batch_y.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
    "        loss = criterion(pred_y, y_train_cuda)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # print(f\"Step [{step+1}/{len(train_dataset)}], batch time: {time.time() - since_batch:.2f}, train_loss: {loss.item()}\")\n",
    "\n",
    "    # validation\n",
    "    total_val_loss = 0\n",
    "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
    "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
    "            y_val_cuda = batch_val_y.cuda()\n",
    "        \n",
    "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
    "        val_loss = criterion(pred_y, y_val_cuda)\n",
    "        total_val_loss += val_loss.item()\n",
    "    \n",
    "        # print statistics\n",
    "    if min_val_loss > total_val_loss:\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        min_val_loss = total_val_loss\n",
    "    endtime = datetime.datetime.now()\n",
    "    print('epochs [%d/%d], cost:%.2fs train_loss: %.5f val_loss: %.5f' % \n",
    "          (epoch + 1, epochs, (endtime-starttime).seconds, total_train_loss, total_val_loss))\n",
    "\n",
    "    total_train_loss_list.append(total_train_loss)\n",
    "    total_val_loss_list.append(total_val_loss)\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net.load_state_dict(torch.load(model_name))\n",
    "years = test[5].unique()\n",
    "test_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp = test[test[5]==year]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    test_list.append(temp)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tid_list = []\n",
    "time_list = [] \n",
    "pred_lat_list = []\n",
    "pred_long_list = [] \n",
    "true_lat_list = [] \n",
    "true_long_list = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for year, _test in zip(years, test_list):\n",
    "\n",
    "        print(year, '年:')\n",
    "        # print(\"TID \", _test.loc[:,1])\n",
    "        y_test_lat = _test.loc[:,3]\n",
    "        \n",
    "        y_test_long = _test.loc[:,4]\n",
    "        \n",
    "        X_wide_test = X_wide_scaler.transform(_test.loc[:,6:])\n",
    "\n",
    "        final_test_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_test_dict[scaler_name][reanalysis_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_list.append(X_deep_temp)\n",
    "        X_deep_test = np.concatenate(final_test_list, axis=1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
    "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
    "\n",
    "        \n",
    "        tid  = _test.loc[:,1]\n",
    "        time_ = _test.loc[:,2]\n",
    "        print(\"len(tid) = \",len(tid))\n",
    "        pred = net(X_wide_test, X_deep_test)\n",
    "\n",
    "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy())\n",
    "\n",
    "        pred_lat = pred[:,0]\n",
    "        pred_long = pred[:,1]\n",
    "        \n",
    "        print(\"len(pred_lat) =\", len(pred_lat))\n",
    "        true_lat = y_test_lat\n",
    "        true_long = y_test_long\n",
    "\n",
    "        diff_lat = np.abs(pred_lat - true_lat)\n",
    "        diff_long = np.abs(pred_long - true_long)\n",
    "\n",
    "        print('avg lat:', sum(diff_lat)/len(diff_lat))\n",
    "        print('avg long:', sum(diff_long)/len(diff_long))\n",
    "\n",
    "        sum_error = []\n",
    "        for i in range(0, len(pred_lat)):\n",
    "            sum_error.append(great_circle((pred_lat[i], pred_long[i]), (true_lat[i], true_long[i])).kilometers)\n",
    "\n",
    "        print('avg distance error:', sum(sum_error)/len(sum_error))\n",
    "        \n",
    "        tid_list.append(tid)\n",
    "        time_list.append(time_)\n",
    "        pred_lat_list.append(pred_lat)\n",
    "        pred_long_list.append(pred_long)\n",
    "        true_lat_list.append(true_lat)\n",
    "        true_long_list.append(true_long)\n",
    "        \n",
    "\n",
    "tid_list_ =  [item for sublist in tid_list for item in sublist]\n",
    "time_list_ =  [item for sublist in time_list for item in sublist]\n",
    "pred_lat_list_ =  [item for sublist in pred_lat_list for item in sublist]\n",
    "pred_long_list_ =  [item for sublist in pred_long_list for item in sublist]\n",
    "true_lat_list_ =  [item for sublist in true_lat_list for item in sublist]\n",
    "true_long_list_ =  [item for sublist in true_long_list for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_key = pd.read_csv('../data/raw.csv', header=None)\n",
    "track_data = [] \n",
    "\n",
    "for i in range(len(tid_list_)):\n",
    "\n",
    "    if len(id_key[id_key[0] == str(tid_list_[i])][11].unique()) == 0:\n",
    "\n",
    "        track_data.append([\n",
    "            tid_list_[i], \n",
    "            id_key[id_key[0] == tid_list_[i]][11].unique()[0],\n",
    "            time_list_[i],\n",
    "            true_lat_list_[i],\n",
    "            true_long_list_[i],\n",
    "            pred_lat_list_[i],\n",
    "            pred_long_list_[i]\n",
    "            ])\n",
    "        \n",
    "    else:\n",
    "\n",
    "        track_data.append([\n",
    "            tid_list_[i], \n",
    "            id_key[id_key[0] == str(tid_list_[i])][11].unique()[0],\n",
    "            time_list_[i],\n",
    "            true_lat_list_[i],\n",
    "            true_long_list_[i],\n",
    "            pred_lat_list_[i],\n",
    "            pred_long_list_[i]\n",
    "            ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been written to ./QPA_track_data/track_data_QT_lora_setting_8_ck_768_qnn_depth_20.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predicted trajectory data in the testing dataset\n",
    "import csv\n",
    "\n",
    "file_path = \"../results/QPA_track_data/track_data_QPA_ck_64_qnn_depth_20.csv\"\n",
    "# Define the column headers\n",
    "headers = [\"TID\", \"KEY\", \"TIME\", \"LAT\", \"LONG\", \"PRED_LAT\", \"PRED_LONG\"] \n",
    "\n",
    "# Write to CSV\n",
    "with open(file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # Write the data rows\n",
    "    writer.writerows(track_data)\n",
    "\n",
    "print(f\"CSV file has been written to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
