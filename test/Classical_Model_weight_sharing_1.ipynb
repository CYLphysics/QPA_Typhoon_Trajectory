{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.dirname(current_dir))\n",
    "\n",
    "from code_base.weight_sharing_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)  # Set PyTorch seed\n",
    "    np.random.seed(seed)      # Set NumPy seed\n",
    "    random.seed(seed)         # Set Python random seed\n",
    "\n",
    "    # Ensure reproducibility for CUDA (if using GPU)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable to ensure consistent runs\n",
    "\n",
    "# Set a fixed seed\n",
    "set_seed(58) #56 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "# from torchsummary import summary\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "# forecast 24-hour lead time \n",
    "pre_seq = 4\n",
    "batch_size = 128\n",
    "epochs = 128\n",
    "min_val_loss = 100\n",
    "model_name = '../results/model_saver/Classl_Model_weight_sharing_1.pkl'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 1 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 2 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 3 (8406, 1, 4, 31, 31)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "test = pd.read_csv('../data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "\n",
    "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
    "CLIPER_feature.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_wide_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 6:])\n",
    "X_wide_train = X_wide[0: train.shape[0], :]\n",
    "\n",
    "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3:4])\n",
    "y_train = y[0: train.shape[0], :]\n",
    "\n",
    "reanalysis_type = 'z'\n",
    "\n",
    "# 0 means now \n",
    "# 1 means 6-hour ago\n",
    "# 2 means 12-hour ago\n",
    "ahead_times = [0,1,2,3]\n",
    "pressures = [1000, 750, 500, 250]\n",
    "sequential_reanalysis_list = []\n",
    "reanalysis_test_dict = {}\n",
    "X_deep_scaler_dict = {}\n",
    "\n",
    "\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        \n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "        train_reanalysis_csv = pd.read_csv('../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "        \n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis\n",
    "        \n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        X_deep = X_deep_scaler_dict[scaler_name] .fit_transform(reanalysis.loc[:, 5:])\n",
    "        \n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "    \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_train = np.concatenate(sequential_reanalysis_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "\n",
    "len(train_index), len(val_index)\n",
    "\n",
    "train_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameters in the current model:  4964276\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "print(\"# of trainable parameters in the current model: \",\n",
    "    sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    ")\n",
    "\n",
    "# wo weight sharing: 8399540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [1/128] cost:17.00s train_loss: 8.17645 val_loss: 0.50911\n",
      "epochs [2/128] cost:17.00s train_loss: 3.08029 val_loss: 0.20738\n",
      "epochs [3/128] cost:17.00s train_loss: 1.38346 val_loss: 0.13878\n",
      "epochs [4/128] cost:17.00s train_loss: 1.08606 val_loss: 0.11739\n",
      "epochs [5/128] cost:17.00s train_loss: 1.00837 val_loss: 0.11745\n",
      "epochs [6/128] cost:17.00s train_loss: 0.99551 val_loss: 0.12540\n",
      "epochs [7/128] cost:17.00s train_loss: 0.95597 val_loss: 0.13504\n",
      "epochs [8/128] cost:17.00s train_loss: 0.97118 val_loss: 0.11511\n",
      "epochs [9/128] cost:17.00s train_loss: 0.92220 val_loss: 0.11703\n",
      "epochs [10/128] cost:17.00s train_loss: 0.92156 val_loss: 0.10003\n",
      "epochs [11/128] cost:17.00s train_loss: 0.89002 val_loss: 0.12263\n",
      "epochs [12/128] cost:17.00s train_loss: 0.91467 val_loss: 0.10151\n",
      "epochs [13/128] cost:17.00s train_loss: 0.88873 val_loss: 0.09880\n",
      "epochs [14/128] cost:17.00s train_loss: 0.84044 val_loss: 0.10329\n",
      "epochs [15/128] cost:17.00s train_loss: 0.83536 val_loss: 0.09991\n",
      "epochs [16/128] cost:17.00s train_loss: 0.84688 val_loss: 0.10841\n",
      "epochs [17/128] cost:17.00s train_loss: 0.82597 val_loss: 0.09600\n",
      "epochs [18/128] cost:17.00s train_loss: 0.80952 val_loss: 0.12990\n",
      "epochs [19/128] cost:17.00s train_loss: 0.86442 val_loss: 0.10893\n",
      "epochs [20/128] cost:17.00s train_loss: 0.80002 val_loss: 0.09008\n",
      "epochs [21/128] cost:17.00s train_loss: 0.77871 val_loss: 0.08778\n",
      "epochs [22/128] cost:17.00s train_loss: 0.73013 val_loss: 0.08460\n",
      "epochs [23/128] cost:17.00s train_loss: 0.81077 val_loss: 0.10278\n",
      "epochs [24/128] cost:17.00s train_loss: 0.85111 val_loss: 0.09706\n",
      "epochs [25/128] cost:17.00s train_loss: 0.77903 val_loss: 0.08826\n",
      "epochs [26/128] cost:17.00s train_loss: 0.76258 val_loss: 0.09267\n",
      "epochs [27/128] cost:17.00s train_loss: 0.81702 val_loss: 0.11900\n",
      "epochs [28/128] cost:17.00s train_loss: 0.81412 val_loss: 0.10375\n",
      "epochs [29/128] cost:17.00s train_loss: 0.76892 val_loss: 0.08847\n",
      "epochs [30/128] cost:17.00s train_loss: 0.75239 val_loss: 0.10895\n",
      "epochs [31/128] cost:17.00s train_loss: 0.76502 val_loss: 0.08318\n",
      "epochs [32/128] cost:17.00s train_loss: 0.71693 val_loss: 0.08944\n",
      "epochs [33/128] cost:17.00s train_loss: 0.70490 val_loss: 0.09913\n",
      "epochs [34/128] cost:17.00s train_loss: 0.78336 val_loss: 0.07978\n",
      "epochs [35/128] cost:17.00s train_loss: 0.70226 val_loss: 0.07615\n",
      "epochs [36/128] cost:17.00s train_loss: 0.67469 val_loss: 0.08926\n",
      "epochs [37/128] cost:17.00s train_loss: 0.68788 val_loss: 0.08875\n",
      "epochs [38/128] cost:17.00s train_loss: 0.70299 val_loss: 0.08197\n",
      "epochs [39/128] cost:17.00s train_loss: 0.68196 val_loss: 0.08596\n",
      "epochs [40/128] cost:17.00s train_loss: 0.62742 val_loss: 0.07873\n",
      "epochs [41/128] cost:17.00s train_loss: 0.63332 val_loss: 0.07305\n",
      "epochs [42/128] cost:17.00s train_loss: 0.64384 val_loss: 0.09723\n",
      "epochs [43/128] cost:17.00s train_loss: 0.64685 val_loss: 0.07116\n",
      "epochs [44/128] cost:17.00s train_loss: 0.63982 val_loss: 0.08053\n",
      "epochs [45/128] cost:17.00s train_loss: 0.65903 val_loss: 0.07271\n",
      "epochs [46/128] cost:17.00s train_loss: 0.59937 val_loss: 0.07887\n",
      "epochs [47/128] cost:17.00s train_loss: 0.61300 val_loss: 0.07468\n",
      "epochs [48/128] cost:17.00s train_loss: 0.62349 val_loss: 0.07880\n",
      "epochs [49/128] cost:17.00s train_loss: 0.62399 val_loss: 0.07506\n",
      "epochs [50/128] cost:17.00s train_loss: 0.60768 val_loss: 0.06648\n",
      "epochs [51/128] cost:17.00s train_loss: 0.59322 val_loss: 0.09441\n",
      "epochs [52/128] cost:17.00s train_loss: 0.61134 val_loss: 0.07868\n",
      "epochs [53/128] cost:17.00s train_loss: 0.58941 val_loss: 0.08535\n",
      "epochs [54/128] cost:17.00s train_loss: 0.61022 val_loss: 0.06924\n",
      "epochs [55/128] cost:17.00s train_loss: 0.57237 val_loss: 0.06770\n",
      "epochs [56/128] cost:17.00s train_loss: 0.56253 val_loss: 0.06594\n",
      "epochs [57/128] cost:17.00s train_loss: 0.60471 val_loss: 0.07095\n",
      "epochs [58/128] cost:17.00s train_loss: 0.56932 val_loss: 0.06759\n",
      "epochs [59/128] cost:17.00s train_loss: 0.55335 val_loss: 0.06426\n",
      "epochs [60/128] cost:17.00s train_loss: 0.56543 val_loss: 0.06823\n",
      "epochs [61/128] cost:17.00s train_loss: 0.56508 val_loss: 0.08096\n",
      "epochs [62/128] cost:17.00s train_loss: 0.61464 val_loss: 0.07347\n",
      "epochs [63/128] cost:17.00s train_loss: 0.57055 val_loss: 0.06874\n",
      "epochs [64/128] cost:17.00s train_loss: 0.55744 val_loss: 0.06296\n",
      "epochs [65/128] cost:17.00s train_loss: 0.53728 val_loss: 0.06952\n",
      "epochs [66/128] cost:17.00s train_loss: 0.55722 val_loss: 0.07263\n",
      "epochs [67/128] cost:17.00s train_loss: 0.56691 val_loss: 0.07050\n",
      "epochs [68/128] cost:17.00s train_loss: 0.55038 val_loss: 0.07132\n",
      "epochs [69/128] cost:17.00s train_loss: 0.56032 val_loss: 0.06009\n",
      "epochs [70/128] cost:17.00s train_loss: 0.53771 val_loss: 0.07874\n",
      "epochs [71/128] cost:17.00s train_loss: 0.55261 val_loss: 0.06774\n",
      "epochs [72/128] cost:17.00s train_loss: 0.54357 val_loss: 0.07284\n",
      "epochs [73/128] cost:17.00s train_loss: 0.55412 val_loss: 0.07848\n",
      "epochs [74/128] cost:17.00s train_loss: 0.54910 val_loss: 0.07236\n",
      "epochs [75/128] cost:17.00s train_loss: 0.53321 val_loss: 0.05817\n",
      "epochs [76/128] cost:17.00s train_loss: 0.51652 val_loss: 0.07756\n",
      "epochs [77/128] cost:17.00s train_loss: 0.54999 val_loss: 0.05803\n",
      "epochs [78/128] cost:17.00s train_loss: 0.53277 val_loss: 0.06794\n",
      "epochs [79/128] cost:17.00s train_loss: 0.49576 val_loss: 0.06420\n",
      "epochs [80/128] cost:17.00s train_loss: 0.52448 val_loss: 0.06373\n",
      "epochs [81/128] cost:17.00s train_loss: 0.49251 val_loss: 0.06311\n",
      "epochs [82/128] cost:17.00s train_loss: 0.51787 val_loss: 0.06815\n",
      "epochs [83/128] cost:17.00s train_loss: 0.53031 val_loss: 0.05703\n",
      "epochs [84/128] cost:17.00s train_loss: 0.49182 val_loss: 0.06254\n",
      "epochs [85/128] cost:17.00s train_loss: 0.49274 val_loss: 0.06138\n",
      "epochs [86/128] cost:17.00s train_loss: 0.50227 val_loss: 0.07776\n",
      "epochs [87/128] cost:17.00s train_loss: 0.53334 val_loss: 0.05444\n",
      "epochs [88/128] cost:17.00s train_loss: 0.49892 val_loss: 0.06081\n",
      "epochs [89/128] cost:17.00s train_loss: 0.48512 val_loss: 0.05665\n",
      "epochs [90/128] cost:17.00s train_loss: 0.47300 val_loss: 0.05896\n",
      "epochs [91/128] cost:17.00s train_loss: 0.47073 val_loss: 0.06335\n",
      "epochs [92/128] cost:17.00s train_loss: 0.48620 val_loss: 0.07696\n",
      "epochs [93/128] cost:17.00s train_loss: 0.48424 val_loss: 0.08208\n",
      "epochs [94/128] cost:17.00s train_loss: 0.51126 val_loss: 0.12318\n",
      "epochs [95/128] cost:17.00s train_loss: 0.57959 val_loss: 0.05575\n",
      "epochs [96/128] cost:17.00s train_loss: 0.50675 val_loss: 0.05739\n",
      "epochs [97/128] cost:17.00s train_loss: 0.47464 val_loss: 0.06418\n",
      "epochs [98/128] cost:17.00s train_loss: 0.47928 val_loss: 0.05974\n",
      "epochs [99/128] cost:17.00s train_loss: 0.48896 val_loss: 0.07570\n",
      "epochs [100/128] cost:17.00s train_loss: 0.47392 val_loss: 0.06620\n",
      "epochs [101/128] cost:17.00s train_loss: 0.47762 val_loss: 0.05826\n",
      "epochs [102/128] cost:17.00s train_loss: 0.47482 val_loss: 0.06776\n",
      "epochs [103/128] cost:17.00s train_loss: 0.45584 val_loss: 0.06113\n",
      "epochs [104/128] cost:17.00s train_loss: 0.46941 val_loss: 0.04946\n",
      "epochs [105/128] cost:17.00s train_loss: 0.43848 val_loss: 0.06479\n",
      "epochs [106/128] cost:17.00s train_loss: 0.48426 val_loss: 0.05767\n",
      "epochs [107/128] cost:17.00s train_loss: 0.44154 val_loss: 0.06082\n",
      "epochs [108/128] cost:17.00s train_loss: 0.44762 val_loss: 0.05652\n",
      "epochs [109/128] cost:17.00s train_loss: 0.42101 val_loss: 0.04645\n",
      "epochs [110/128] cost:17.00s train_loss: 0.44446 val_loss: 0.05632\n",
      "epochs [111/128] cost:17.00s train_loss: 0.44314 val_loss: 0.05870\n",
      "epochs [112/128] cost:17.00s train_loss: 0.45006 val_loss: 0.06409\n",
      "epochs [113/128] cost:17.00s train_loss: 0.42373 val_loss: 0.06344\n",
      "epochs [114/128] cost:17.00s train_loss: 0.42010 val_loss: 0.04694\n",
      "epochs [115/128] cost:17.00s train_loss: 0.42953 val_loss: 0.04928\n",
      "epochs [116/128] cost:17.00s train_loss: 0.41711 val_loss: 0.04726\n",
      "epochs [117/128] cost:17.00s train_loss: 0.41478 val_loss: 0.07325\n",
      "epochs [118/128] cost:17.00s train_loss: 0.44254 val_loss: 0.05742\n",
      "epochs [119/128] cost:17.00s train_loss: 0.41187 val_loss: 0.05458\n",
      "epochs [120/128] cost:17.00s train_loss: 0.39892 val_loss: 0.05001\n",
      "epochs [121/128] cost:17.00s train_loss: 0.41959 val_loss: 0.05734\n",
      "epochs [122/128] cost:17.00s train_loss: 0.39540 val_loss: 0.05375\n",
      "epochs [123/128] cost:17.00s train_loss: 0.41422 val_loss: 0.05792\n",
      "epochs [124/128] cost:17.00s train_loss: 0.42462 val_loss: 0.04468\n",
      "epochs [125/128] cost:17.00s train_loss: 0.40031 val_loss: 0.05309\n",
      "epochs [126/128] cost:17.00s train_loss: 0.38978 val_loss: 0.06108\n",
      "epochs [127/128] cost:17.00s train_loss: 0.39356 val_loss: 0.05705\n",
      "epochs [128/128] cost:17.00s train_loss: 0.38766 val_loss: 0.06791\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    starttime = datetime.datetime.now()\n",
    "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "    train_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    val_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    # training\n",
    "    total_train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "            X_wide_train_cuda = batch_x[0].float().cuda()\n",
    "            X_deep_train_cuda = batch_x[1].float().cuda()\n",
    "            y_train_cuda = batch_y.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
    "        loss = criterion(pred_y, y_train_cuda)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # validation\n",
    "    total_val_loss = 0\n",
    "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
    "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
    "            y_val_cuda = batch_val_y.cuda()\n",
    "        \n",
    "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
    "        val_loss = criterion(pred_y, y_val_cuda)\n",
    "        total_val_loss += val_loss.item()\n",
    "    \n",
    "        # print statistics\n",
    "    if min_val_loss > total_val_loss:\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        min_val_loss = total_val_loss\n",
    "    endtime = datetime.datetime.now()\n",
    "    print('epochs [%d/%d] cost:%.2fs train_loss: %.5f val_loss: %.5f' % \n",
    "          (epoch + 1, epochs, (endtime-starttime).seconds, total_train_loss, total_val_loss))\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = test[5].unique()\n",
    "test_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp = test[test[5]==year]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    test_list.append(temp)\n",
    "    \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2015\n",
      "len(tid) =  908\n",
      "len(pred_lat) = 908\n",
      "avg lat: 0.7601423190028656\n",
      "avg long: 0.9298767241087241\n",
      "avg distance error: 138.34627224805465\n",
      "Year: 2016\n",
      "len(tid) =  489\n",
      "len(pred_lat) = 489\n",
      "avg lat: 0.8517363261591433\n",
      "avg long: 0.9880436559883833\n",
      "avg distance error: 148.74779501243424\n",
      "Year: 2017\n",
      "len(tid) =  544\n",
      "len(pred_lat) = 544\n",
      "avg lat: 0.8153262217255197\n",
      "avg long: 1.0541587801540604\n",
      "avg distance error: 153.21660248050534\n",
      "Year: 2018\n",
      "len(tid) =  806\n",
      "len(pred_lat) = 806\n",
      "avg lat: 0.8926407771430007\n",
      "avg long: 1.1295992754233382\n",
      "avg distance error: 164.3510691757483\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tid_list = []\n",
    "time_list = [] \n",
    "pred_lat_list = []\n",
    "pred_long_list = [] \n",
    "true_lat_list = [] \n",
    "true_long_list = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for year, _test in zip(years, test_list):\n",
    "\n",
    "        print('Year:', year)\n",
    "        # print(\"TID \", _test.loc[:,1])\n",
    "        y_test_lat = _test.loc[:,3]\n",
    "        \n",
    "        y_test_long = _test.loc[:,4]\n",
    "        \n",
    "        X_wide_test = X_wide_scaler.transform(_test.loc[:,6:])\n",
    "\n",
    "        final_test_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_test_dict[scaler_name][reanalysis_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_list.append(X_deep_temp)\n",
    "        X_deep_test = np.concatenate(final_test_list, axis=1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
    "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
    "\n",
    "        \n",
    "        tid  = _test.loc[:,1]\n",
    "        time_ = _test.loc[:,2]\n",
    "        print(\"len(tid) = \",len(tid))\n",
    "        pred = net(X_wide_test, X_deep_test)\n",
    "\n",
    "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy())\n",
    "\n",
    "        pred_lat = pred[:,0]\n",
    "        pred_long = pred[:,1]\n",
    "        \n",
    "        print(\"len(pred_lat) =\", len(pred_lat))\n",
    "        true_lat = y_test_lat\n",
    "        true_long = y_test_long\n",
    "\n",
    "        diff_lat = np.abs(pred_lat - true_lat)\n",
    "        diff_long = np.abs(pred_long - true_long)\n",
    "\n",
    "        print('avg lat:', sum(diff_lat)/len(diff_lat))\n",
    "        print('avg long:', sum(diff_long)/len(diff_long))\n",
    "\n",
    "        sum_error = []\n",
    "        for i in range(0, len(pred_lat)):\n",
    "            sum_error.append(great_circle((pred_lat[i], pred_long[i]), (true_lat[i], true_long[i])).kilometers)\n",
    "\n",
    "        print('avg distance error:', sum(sum_error)/len(sum_error))\n",
    "        \n",
    "        tid_list.append(tid)\n",
    "        time_list.append(time_)\n",
    "        pred_lat_list.append(pred_lat)\n",
    "        pred_long_list.append(pred_long)\n",
    "        true_lat_list.append(true_lat)\n",
    "        true_long_list.append(true_long)\n",
    "        \n",
    "\n",
    "tid_list_ =  [item for sublist in tid_list for item in sublist]\n",
    "time_list_ =  [item for sublist in time_list for item in sublist]\n",
    "pred_lat_list_ =  [item for sublist in pred_lat_list for item in sublist]\n",
    "pred_long_list_ =  [item for sublist in pred_long_list for item in sublist]\n",
    "true_lat_list_ =  [item for sublist in true_lat_list for item in sublist]\n",
    "true_long_list_ =  [item for sublist in true_long_list for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118485/2693083039.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  id_key = pd.read_csv('../data/raw.csv', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been written to ../results/QPA_track_data/track_data_classical_weight_sharing_1.csv\n"
     ]
    }
   ],
   "source": [
    "id_key = pd.read_csv('../data/raw.csv', header=None)\n",
    "track_data = [] \n",
    "\n",
    "for i in range(len(tid_list_)):\n",
    "\n",
    "    if len(id_key[id_key[0] == str(tid_list_[i])][11].unique()) == 0:\n",
    "\n",
    "        track_data.append([\n",
    "            tid_list_[i], \n",
    "            id_key[id_key[0] == tid_list_[i]][11].unique()[0],\n",
    "            time_list_[i],\n",
    "            true_lat_list_[i],\n",
    "            true_long_list_[i],\n",
    "            pred_lat_list_[i],\n",
    "            pred_long_list_[i]\n",
    "            ])\n",
    "        \n",
    "    else:\n",
    "\n",
    "        track_data.append([\n",
    "            tid_list_[i], \n",
    "            id_key[id_key[0] == str(tid_list_[i])][11].unique()[0],\n",
    "            time_list_[i],\n",
    "            true_lat_list_[i],\n",
    "            true_long_list_[i],\n",
    "            pred_lat_list_[i],\n",
    "            pred_long_list_[i]\n",
    "            ])\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "file_path = \"../results/QPA_track_data/track_data_classical_weight_sharing_1.csv\"\n",
    "# Define the column headers\n",
    "headers = [\"TID\", \"KEY\", \"TIME\", \"LAT\", \"LONG\", \"PRED_LAT\", \"PRED_LONG\"] \n",
    "\n",
    "# Write to CSV\n",
    "with open(file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # Write the data rows\n",
    "    writer.writerows(track_data)\n",
    "\n",
    "print(f\"CSV file has been written to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
