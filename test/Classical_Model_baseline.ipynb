{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.dirname(current_dir))\n",
    "\n",
    "from code_base.classical_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "# from torchsummary import summary\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "# forecast 24-hour lead time \n",
    "pre_seq = 4\n",
    "batch_size = 128\n",
    "epochs = 128\n",
    "min_val_loss = 100\n",
    "model_name = '../results/model_saver/Model.pkl'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 1 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 2 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 3 (8406, 1, 4, 31, 31)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "test = pd.read_csv('../data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "\n",
    "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
    "CLIPER_feature.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_wide_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 6:])\n",
    "X_wide_train = X_wide[0: train.shape[0], :]\n",
    "\n",
    "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3:4])\n",
    "y_train = y[0: train.shape[0], :]\n",
    "\n",
    "reanalysis_type = 'z'\n",
    "\n",
    "# 0 means now \n",
    "# 1 means 6-hour ago\n",
    "# 2 means 12-hour ago\n",
    "ahead_times = [0,1,2,3]\n",
    "pressures = [1000, 750, 500, 250]\n",
    "sequential_reanalysis_list = []\n",
    "reanalysis_test_dict = {}\n",
    "X_deep_scaler_dict = {}\n",
    "\n",
    "\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        \n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "        train_reanalysis_csv = pd.read_csv('../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "        \n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis\n",
    "        \n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        X_deep = X_deep_scaler_dict[scaler_name] .fit_transform(reanalysis.loc[:, 5:])\n",
    "        \n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "    \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_train = np.concatenate(sequential_reanalysis_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "\n",
    "len(train_index), len(val_index)\n",
    "\n",
    "train_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameters in the current model:  8399540\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "print(\"# of trainable parameters in the current model: \",\n",
    "    sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [1/128] cost:16.00s train_loss: 7.97345 val_loss: 0.44923\n",
      "epochs [2/128] cost:16.00s train_loss: 2.47582 val_loss: 0.19677\n",
      "epochs [3/128] cost:17.00s train_loss: 1.22692 val_loss: 0.12256\n",
      "epochs [4/128] cost:17.00s train_loss: 1.02168 val_loss: 0.21590\n",
      "epochs [5/128] cost:17.00s train_loss: 1.07046 val_loss: 0.10985\n",
      "epochs [6/128] cost:17.00s train_loss: 0.96178 val_loss: 0.14546\n",
      "epochs [7/128] cost:17.00s train_loss: 1.01239 val_loss: 0.10793\n",
      "epochs [8/128] cost:17.00s train_loss: 0.92969 val_loss: 0.10071\n",
      "epochs [9/128] cost:17.00s train_loss: 0.91529 val_loss: 0.10595\n",
      "epochs [10/128] cost:17.00s train_loss: 0.93694 val_loss: 0.09846\n",
      "epochs [11/128] cost:17.00s train_loss: 0.92103 val_loss: 0.11082\n",
      "epochs [12/128] cost:17.00s train_loss: 0.88487 val_loss: 0.11222\n",
      "epochs [13/128] cost:17.00s train_loss: 0.86906 val_loss: 0.10801\n",
      "epochs [14/128] cost:17.00s train_loss: 0.86839 val_loss: 0.10598\n",
      "epochs [15/128] cost:17.00s train_loss: 0.85239 val_loss: 0.10330\n",
      "epochs [16/128] cost:17.00s train_loss: 0.83143 val_loss: 0.09792\n",
      "epochs [17/128] cost:17.00s train_loss: 0.87881 val_loss: 0.10455\n",
      "epochs [18/128] cost:17.00s train_loss: 0.85958 val_loss: 0.09647\n",
      "epochs [19/128] cost:17.00s train_loss: 0.84695 val_loss: 0.12239\n",
      "epochs [20/128] cost:17.00s train_loss: 0.85869 val_loss: 0.12034\n",
      "epochs [21/128] cost:17.00s train_loss: 0.83976 val_loss: 0.09510\n",
      "epochs [22/128] cost:17.00s train_loss: 0.81574 val_loss: 0.08994\n",
      "epochs [23/128] cost:17.00s train_loss: 0.81097 val_loss: 0.09611\n",
      "epochs [24/128] cost:17.00s train_loss: 0.81076 val_loss: 0.11084\n",
      "epochs [25/128] cost:17.00s train_loss: 0.82711 val_loss: 0.10297\n",
      "epochs [26/128] cost:17.00s train_loss: 0.79713 val_loss: 0.08816\n",
      "epochs [27/128] cost:17.00s train_loss: 0.76999 val_loss: 0.09612\n",
      "epochs [28/128] cost:17.00s train_loss: 0.78879 val_loss: 0.10318\n",
      "epochs [29/128] cost:17.00s train_loss: 0.79741 val_loss: 0.09410\n",
      "epochs [30/128] cost:17.00s train_loss: 0.76923 val_loss: 0.09329\n",
      "epochs [31/128] cost:17.00s train_loss: 0.74592 val_loss: 0.09721\n",
      "epochs [32/128] cost:17.00s train_loss: 0.77494 val_loss: 0.09833\n",
      "epochs [33/128] cost:17.00s train_loss: 0.75155 val_loss: 0.10604\n",
      "epochs [34/128] cost:17.00s train_loss: 0.76478 val_loss: 0.08317\n",
      "epochs [35/128] cost:17.00s train_loss: 0.71359 val_loss: 0.15193\n",
      "epochs [36/128] cost:17.00s train_loss: 0.79192 val_loss: 0.10805\n",
      "epochs [37/128] cost:17.00s train_loss: 0.74772 val_loss: 0.08382\n",
      "epochs [38/128] cost:17.00s train_loss: 0.74109 val_loss: 0.07971\n",
      "epochs [39/128] cost:17.00s train_loss: 0.71082 val_loss: 0.09324\n",
      "epochs [40/128] cost:17.00s train_loss: 0.73161 val_loss: 0.08925\n",
      "epochs [41/128] cost:17.00s train_loss: 0.72503 val_loss: 0.07537\n",
      "epochs [42/128] cost:17.00s train_loss: 0.70216 val_loss: 0.07512\n",
      "epochs [43/128] cost:17.00s train_loss: 0.66808 val_loss: 0.08126\n",
      "epochs [44/128] cost:17.00s train_loss: 0.66694 val_loss: 0.08332\n",
      "epochs [45/128] cost:17.00s train_loss: 0.68372 val_loss: 0.08882\n",
      "epochs [46/128] cost:17.00s train_loss: 0.66839 val_loss: 0.07331\n",
      "epochs [47/128] cost:17.00s train_loss: 0.66097 val_loss: 0.10017\n",
      "epochs [48/128] cost:17.00s train_loss: 0.70557 val_loss: 0.07774\n",
      "epochs [49/128] cost:17.00s train_loss: 0.69338 val_loss: 0.09035\n",
      "epochs [50/128] cost:17.00s train_loss: 0.70019 val_loss: 0.07494\n",
      "epochs [51/128] cost:17.00s train_loss: 0.66800 val_loss: 0.07717\n",
      "epochs [52/128] cost:17.00s train_loss: 0.63974 val_loss: 0.07950\n",
      "epochs [53/128] cost:17.00s train_loss: 0.66501 val_loss: 0.07537\n",
      "epochs [54/128] cost:17.00s train_loss: 0.61066 val_loss: 0.06890\n",
      "epochs [55/128] cost:17.00s train_loss: 0.59744 val_loss: 0.10713\n",
      "epochs [56/128] cost:17.00s train_loss: 0.60802 val_loss: 0.09923\n",
      "epochs [57/128] cost:17.00s train_loss: 0.62650 val_loss: 0.08923\n",
      "epochs [58/128] cost:17.00s train_loss: 0.62577 val_loss: 0.07102\n",
      "epochs [59/128] cost:17.00s train_loss: 0.60698 val_loss: 0.08069\n",
      "epochs [60/128] cost:17.00s train_loss: 0.64358 val_loss: 0.08281\n",
      "epochs [61/128] cost:17.00s train_loss: 0.57873 val_loss: 0.07281\n",
      "epochs [62/128] cost:17.00s train_loss: 0.59347 val_loss: 0.07079\n",
      "epochs [63/128] cost:17.00s train_loss: 0.55174 val_loss: 0.07351\n",
      "epochs [64/128] cost:17.00s train_loss: 0.57860 val_loss: 0.07061\n",
      "epochs [65/128] cost:17.00s train_loss: 0.56037 val_loss: 0.11510\n",
      "epochs [66/128] cost:17.00s train_loss: 0.62191 val_loss: 0.06916\n",
      "epochs [67/128] cost:17.00s train_loss: 0.55337 val_loss: 0.06430\n",
      "epochs [68/128] cost:17.00s train_loss: 0.56288 val_loss: 0.06610\n",
      "epochs [69/128] cost:17.00s train_loss: 0.56097 val_loss: 0.06478\n",
      "epochs [70/128] cost:17.00s train_loss: 0.54423 val_loss: 0.06957\n",
      "epochs [71/128] cost:18.00s train_loss: 0.52651 val_loss: 0.06995\n",
      "epochs [72/128] cost:17.00s train_loss: 0.50304 val_loss: 0.06270\n",
      "epochs [73/128] cost:17.00s train_loss: 0.50521 val_loss: 0.06988\n",
      "epochs [74/128] cost:17.00s train_loss: 0.54890 val_loss: 0.07281\n",
      "epochs [75/128] cost:17.00s train_loss: 0.55242 val_loss: 0.05966\n",
      "epochs [76/128] cost:17.00s train_loss: 0.52322 val_loss: 0.06454\n",
      "epochs [77/128] cost:17.00s train_loss: 0.50393 val_loss: 0.06415\n",
      "epochs [78/128] cost:17.00s train_loss: 0.52325 val_loss: 0.06833\n",
      "epochs [79/128] cost:17.00s train_loss: 0.53190 val_loss: 0.06072\n",
      "epochs [80/128] cost:17.00s train_loss: 0.51614 val_loss: 0.06811\n",
      "epochs [81/128] cost:17.00s train_loss: 0.49187 val_loss: 0.06178\n",
      "epochs [82/128] cost:17.00s train_loss: 0.50134 val_loss: 0.08739\n",
      "epochs [83/128] cost:17.00s train_loss: 0.53288 val_loss: 0.06056\n",
      "epochs [84/128] cost:17.00s train_loss: 0.52764 val_loss: 0.06482\n",
      "epochs [85/128] cost:17.00s train_loss: 0.49855 val_loss: 0.08454\n",
      "epochs [86/128] cost:17.00s train_loss: 0.54074 val_loss: 0.06531\n",
      "epochs [87/128] cost:17.00s train_loss: 0.48395 val_loss: 0.06855\n",
      "epochs [88/128] cost:17.00s train_loss: 0.50130 val_loss: 0.05611\n",
      "epochs [89/128] cost:17.00s train_loss: 0.47878 val_loss: 0.07148\n",
      "epochs [90/128] cost:17.00s train_loss: 0.49229 val_loss: 0.05829\n",
      "epochs [91/128] cost:17.00s train_loss: 0.47767 val_loss: 0.05234\n",
      "epochs [92/128] cost:17.00s train_loss: 0.44542 val_loss: 0.07290\n",
      "epochs [93/128] cost:17.00s train_loss: 0.51024 val_loss: 0.05958\n",
      "epochs [94/128] cost:17.00s train_loss: 0.48938 val_loss: 0.05386\n",
      "epochs [95/128] cost:17.00s train_loss: 0.46129 val_loss: 0.05125\n",
      "epochs [96/128] cost:17.00s train_loss: 0.44928 val_loss: 0.05202\n",
      "epochs [97/128] cost:17.00s train_loss: 0.45766 val_loss: 0.05272\n",
      "epochs [98/128] cost:17.00s train_loss: 0.45439 val_loss: 0.05810\n",
      "epochs [99/128] cost:17.00s train_loss: 0.46053 val_loss: 0.06966\n",
      "epochs [100/128] cost:17.00s train_loss: 0.45930 val_loss: 0.05488\n",
      "epochs [101/128] cost:17.00s train_loss: 0.43428 val_loss: 0.06016\n",
      "epochs [102/128] cost:17.00s train_loss: 0.43891 val_loss: 0.05120\n",
      "epochs [103/128] cost:17.00s train_loss: 0.43546 val_loss: 0.05791\n",
      "epochs [104/128] cost:17.00s train_loss: 0.44385 val_loss: 0.05149\n",
      "epochs [105/128] cost:17.00s train_loss: 0.42447 val_loss: 0.05201\n",
      "epochs [106/128] cost:17.00s train_loss: 0.40817 val_loss: 0.06633\n",
      "epochs [107/128] cost:17.00s train_loss: 0.44183 val_loss: 0.05887\n",
      "epochs [108/128] cost:17.00s train_loss: 0.44556 val_loss: 0.07732\n",
      "epochs [109/128] cost:17.00s train_loss: 0.47223 val_loss: 0.06271\n",
      "epochs [110/128] cost:17.00s train_loss: 0.44673 val_loss: 0.05208\n",
      "epochs [111/128] cost:17.00s train_loss: 0.41994 val_loss: 0.04957\n",
      "epochs [112/128] cost:17.00s train_loss: 0.41399 val_loss: 0.05760\n",
      "epochs [113/128] cost:17.00s train_loss: 0.41088 val_loss: 0.05227\n",
      "epochs [114/128] cost:17.00s train_loss: 0.40572 val_loss: 0.05067\n",
      "epochs [115/128] cost:17.00s train_loss: 0.42539 val_loss: 0.07084\n",
      "epochs [116/128] cost:17.00s train_loss: 0.42739 val_loss: 0.05305\n",
      "epochs [117/128] cost:17.00s train_loss: 0.42892 val_loss: 0.04970\n",
      "epochs [118/128] cost:17.00s train_loss: 0.40293 val_loss: 0.05022\n",
      "epochs [119/128] cost:17.00s train_loss: 0.39474 val_loss: 0.04985\n",
      "epochs [120/128] cost:17.00s train_loss: 0.41640 val_loss: 0.04373\n",
      "epochs [121/128] cost:17.00s train_loss: 0.37377 val_loss: 0.04566\n",
      "epochs [122/128] cost:17.00s train_loss: 0.39415 val_loss: 0.04422\n",
      "epochs [123/128] cost:17.00s train_loss: 0.50516 val_loss: 0.11568\n",
      "epochs [124/128] cost:15.00s train_loss: 15.69909 val_loss: 1.98302\n",
      "epochs [125/128] cost:15.00s train_loss: 16.99691 val_loss: 1.99631\n",
      "epochs [126/128] cost:15.00s train_loss: 16.92311 val_loss: 2.04303\n",
      "epochs [127/128] cost:15.00s train_loss: 16.92242 val_loss: 1.99153\n",
      "epochs [128/128] cost:15.00s train_loss: 16.94519 val_loss: 2.01370\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    starttime = datetime.datetime.now()\n",
    "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "    train_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    val_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    # training\n",
    "    total_train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "            X_wide_train_cuda = batch_x[0].float().cuda()\n",
    "            X_deep_train_cuda = batch_x[1].float().cuda()\n",
    "            y_train_cuda = batch_y.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
    "        loss = criterion(pred_y, y_train_cuda)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # validation\n",
    "    total_val_loss = 0\n",
    "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
    "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
    "            y_val_cuda = batch_val_y.cuda()\n",
    "        \n",
    "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
    "        val_loss = criterion(pred_y, y_val_cuda)\n",
    "        total_val_loss += val_loss.item()\n",
    "    \n",
    "        # print statistics\n",
    "    if min_val_loss > total_val_loss:\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        min_val_loss = total_val_loss\n",
    "    endtime = datetime.datetime.now()\n",
    "    print('epochs [%d/%d] cost:%.2fs train_loss: %.5f val_loss: %.5f' % \n",
    "          (epoch + 1, epochs, (endtime-starttime).seconds, total_train_loss, total_val_loss))\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = test[5].unique()\n",
    "test_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp = test[test[5]==year]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    test_list.append(temp)\n",
    "    \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2015\n",
      "len(tid) =  908\n",
      "len(pred_lat) = 908\n",
      "avg lat: 0.7761790395308169\n",
      "avg long: 1.124004385859952\n",
      "avg distance error: 151.11087071837528\n",
      "Year: 2016\n",
      "len(tid) =  489\n",
      "len(pred_lat) = 489\n",
      "avg lat: 0.926850390873073\n",
      "avg long: 0.9554687531204057\n",
      "avg distance error: 155.0241224771872\n",
      "Year: 2017\n",
      "len(tid) =  544\n",
      "len(pred_lat) = 544\n",
      "avg lat: 0.7922778296120022\n",
      "avg long: 1.1255125550662757\n",
      "avg distance error: 155.11974186267523\n",
      "Year: 2018\n",
      "len(tid) =  806\n",
      "len(pred_lat) = 806\n",
      "avg lat: 0.9331827371942779\n",
      "avg long: 1.2115685380126338\n",
      "avg distance error: 172.39881814313662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tid_list = []\n",
    "time_list = [] \n",
    "pred_lat_list = []\n",
    "pred_long_list = [] \n",
    "true_lat_list = [] \n",
    "true_long_list = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for year, _test in zip(years, test_list):\n",
    "\n",
    "        print('Year:', year)\n",
    "        # print(\"TID \", _test.loc[:,1])\n",
    "        y_test_lat = _test.loc[:,3]\n",
    "        \n",
    "        y_test_long = _test.loc[:,4]\n",
    "        \n",
    "        X_wide_test = X_wide_scaler.transform(_test.loc[:,6:])\n",
    "\n",
    "        final_test_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_test_dict[scaler_name][reanalysis_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_list.append(X_deep_temp)\n",
    "        X_deep_test = np.concatenate(final_test_list, axis=1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
    "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
    "\n",
    "        \n",
    "        tid  = _test.loc[:,1]\n",
    "        time_ = _test.loc[:,2]\n",
    "        print(\"len(tid) = \",len(tid))\n",
    "        pred = net(X_wide_test, X_deep_test)\n",
    "\n",
    "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy())\n",
    "\n",
    "        pred_lat = pred[:,0]\n",
    "        pred_long = pred[:,1]\n",
    "        \n",
    "        print(\"len(pred_lat) =\", len(pred_lat))\n",
    "        true_lat = y_test_lat\n",
    "        true_long = y_test_long\n",
    "\n",
    "        diff_lat = np.abs(pred_lat - true_lat)\n",
    "        diff_long = np.abs(pred_long - true_long)\n",
    "\n",
    "        print('avg lat:', sum(diff_lat)/len(diff_lat))\n",
    "        print('avg long:', sum(diff_long)/len(diff_long))\n",
    "\n",
    "        sum_error = []\n",
    "        for i in range(0, len(pred_lat)):\n",
    "            sum_error.append(great_circle((pred_lat[i], pred_long[i]), (true_lat[i], true_long[i])).kilometers)\n",
    "\n",
    "        print('avg distance error:', sum(sum_error)/len(sum_error))\n",
    "        \n",
    "        tid_list.append(tid)\n",
    "        time_list.append(time_)\n",
    "        pred_lat_list.append(pred_lat)\n",
    "        pred_long_list.append(pred_long)\n",
    "        true_lat_list.append(true_lat)\n",
    "        true_long_list.append(true_long)\n",
    "        \n",
    "\n",
    "tid_list_ =  [item for sublist in tid_list for item in sublist]\n",
    "time_list_ =  [item for sublist in time_list for item in sublist]\n",
    "pred_lat_list_ =  [item for sublist in pred_lat_list for item in sublist]\n",
    "pred_long_list_ =  [item for sublist in pred_long_list for item in sublist]\n",
    "true_lat_list_ =  [item for sublist in true_lat_list for item in sublist]\n",
    "true_long_list_ =  [item for sublist in true_long_list for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62112/142811645.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  id_key = pd.read_csv('../data/raw.csv', header=None)\n"
     ]
    }
   ],
   "source": [
    "id_key = pd.read_csv('../data/raw.csv', header=None)\n",
    "track_data = [] \n",
    "\n",
    "for i in range(len(tid_list_)):\n",
    "\n",
    "    if len(id_key[id_key[0] == str(tid_list_[i])][11].unique()) == 0:\n",
    "\n",
    "        track_data.append([\n",
    "            tid_list_[i], \n",
    "            id_key[id_key[0] == tid_list_[i]][11].unique()[0],\n",
    "            time_list_[i],\n",
    "            true_lat_list_[i],\n",
    "            true_long_list_[i],\n",
    "            pred_lat_list_[i],\n",
    "            pred_long_list_[i]\n",
    "            ])\n",
    "        \n",
    "    else:\n",
    "\n",
    "        track_data.append([\n",
    "            tid_list_[i], \n",
    "            id_key[id_key[0] == str(tid_list_[i])][11].unique()[0],\n",
    "            time_list_[i],\n",
    "            true_lat_list_[i],\n",
    "            true_long_list_[i],\n",
    "            pred_lat_list_[i],\n",
    "            pred_long_list_[i]\n",
    "            ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been written to ../results/QPA_track_data/track_data_cy.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predicted trajectory data in the testing dataset\n",
    "import csv\n",
    "\n",
    "file_path = \"../results/QPA_track_data/track_data_cy.csv\"\n",
    "# Define the column headers\n",
    "headers = [\"TID\", \"KEY\", \"TIME\", \"LAT\", \"LONG\", \"PRED_LAT\", \"PRED_LONG\"] \n",
    "\n",
    "# Write to CSV\n",
    "with open(file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # Write the data rows\n",
    "    writer.writerows(track_data)\n",
    "\n",
    "print(f\"CSV file has been written to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
