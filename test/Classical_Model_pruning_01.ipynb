{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.dirname(current_dir))\n",
    "\n",
    "from code_base.classical_utils import * \n",
    "from code_base.pruning_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "# from torchsummary import summary\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "# forecast 24-hour lead time \n",
    "pre_seq = 4\n",
    "batch_size = 128\n",
    "epochs = 128\n",
    "min_val_loss = 100\n",
    "model_name = '../results/model_saver/Classl_Model_pruning_01.pkl'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 1 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 2 (8406, 1, 4, 31, 31)\n",
      "ahead_time: 3 (8406, 1, 4, 31, 31)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "test = pd.read_csv('../data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "\n",
    "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
    "CLIPER_feature.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_wide_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 6:])\n",
    "X_wide_train = X_wide[0: train.shape[0], :]\n",
    "\n",
    "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3:4])\n",
    "y_train = y[0: train.shape[0], :]\n",
    "\n",
    "reanalysis_type = 'z'\n",
    "\n",
    "# 0 means now \n",
    "# 1 means 6-hour ago\n",
    "# 2 means 12-hour ago\n",
    "ahead_times = [0,1,2,3]\n",
    "pressures = [1000, 750, 500, 250]\n",
    "sequential_reanalysis_list = []\n",
    "reanalysis_test_dict = {}\n",
    "X_deep_scaler_dict = {}\n",
    "\n",
    "\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        \n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "        train_reanalysis_csv = pd.read_csv('../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "        \n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis\n",
    "        \n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        X_deep = X_deep_scaler_dict[scaler_name] .fit_transform(reanalysis.loc[:, 5:])\n",
    "        \n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "    \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_train = np.concatenate(sequential_reanalysis_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "\n",
    "len(train_index), len(val_index)\n",
    "\n",
    "train_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameters in the current model:  8399540\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied structured pruning to Conv2d layer channel_attention.conv_du.1 with 10.0% filters pruned.\n",
      "Applied structured pruning to Conv2d layer channel_attention.conv_du.3 with 10.0% filters pruned.\n",
      "Applied structured pruning to convGRU layer convGRU1.cell with 10.0% neurons pruned.\n",
      "Applied structured pruning to Conv2d layer channel_attention_0.conv_du.1 with 10.0% filters pruned.\n",
      "Applied structured pruning to Conv2d layer channel_attention_0.conv_du.3 with 10.0% filters pruned.\n",
      "Applied structured pruning to convGRU layer convGRU2.cell with 10.0% neurons pruned.\n",
      "Applied structured pruning to Conv2d layer channel_attention_1.conv_du.1 with 10.0% filters pruned.\n",
      "Applied structured pruning to Conv2d layer channel_attention_1.conv_du.3 with 10.0% filters pruned.\n",
      "Applied structured pruning to convGRU layer convGRU3.cell with 10.0% neurons pruned.\n",
      "Applied structured pruning to Conv2d layer channel_attention_2.conv_du.1 with 10.0% filters pruned.\n",
      "Applied structured pruning to Conv2d layer channel_attention_2.conv_du.3 with 10.0% filters pruned.\n",
      "Applied structured pruning to Linear layer fc1 with 10.0% neurons pruned.\n",
      "Applied structured pruning to Linear layer fc2 with 10.0% neurons pruned.\n",
      "Applied structured pruning to Linear layer fc3 with 10.0% neurons pruned.\n",
      "Non-zero parameters: 7558399/8399540 (89.99%)\n"
     ]
    }
   ],
   "source": [
    "apply_pruning(net, amount=0.1)\n",
    "print_nonzero_weights(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = MaskedAdam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [1/128] cost:15.00s train_loss: 8.16747 val_loss: 0.51038\n",
      "epochs [2/128] cost:15.00s train_loss: 2.92057 val_loss: 0.19670\n",
      "epochs [3/128] cost:15.00s train_loss: 1.29510 val_loss: 0.11868\n",
      "epochs [4/128] cost:15.00s train_loss: 1.03659 val_loss: 0.12409\n",
      "epochs [5/128] cost:15.00s train_loss: 0.96493 val_loss: 0.12662\n",
      "epochs [6/128] cost:15.00s train_loss: 0.95822 val_loss: 0.10322\n",
      "epochs [7/128] cost:15.00s train_loss: 0.89030 val_loss: 0.12073\n",
      "epochs [8/128] cost:15.00s train_loss: 0.91847 val_loss: 0.10817\n",
      "epochs [9/128] cost:15.00s train_loss: 0.83575 val_loss: 0.13038\n",
      "epochs [10/128] cost:15.00s train_loss: 0.84105 val_loss: 0.08716\n",
      "epochs [11/128] cost:15.00s train_loss: 0.78716 val_loss: 0.09348\n",
      "epochs [12/128] cost:15.00s train_loss: 0.75439 val_loss: 0.09408\n",
      "epochs [13/128] cost:15.00s train_loss: 0.73949 val_loss: 0.12616\n",
      "epochs [14/128] cost:15.00s train_loss: 0.77162 val_loss: 0.08022\n",
      "epochs [15/128] cost:15.00s train_loss: 0.72287 val_loss: 0.09608\n",
      "epochs [16/128] cost:15.00s train_loss: 0.71926 val_loss: 0.09838\n",
      "epochs [17/128] cost:15.00s train_loss: 0.70388 val_loss: 0.09399\n",
      "epochs [18/128] cost:15.00s train_loss: 0.72384 val_loss: 0.08959\n",
      "epochs [19/128] cost:15.00s train_loss: 0.67040 val_loss: 0.07439\n",
      "epochs [20/128] cost:15.00s train_loss: 0.66430 val_loss: 0.10078\n",
      "epochs [21/128] cost:15.00s train_loss: 0.67935 val_loss: 0.08054\n",
      "epochs [22/128] cost:15.00s train_loss: 0.65697 val_loss: 0.09583\n",
      "epochs [23/128] cost:15.00s train_loss: 0.69055 val_loss: 0.07711\n",
      "epochs [24/128] cost:15.00s train_loss: 0.64810 val_loss: 0.07781\n",
      "epochs [25/128] cost:15.00s train_loss: 0.67307 val_loss: 0.07226\n",
      "epochs [26/128] cost:15.00s train_loss: 0.65635 val_loss: 0.08490\n",
      "epochs [27/128] cost:15.00s train_loss: 0.66898 val_loss: 0.08657\n",
      "epochs [28/128] cost:15.00s train_loss: 0.61514 val_loss: 0.07558\n",
      "epochs [29/128] cost:15.00s train_loss: 0.59994 val_loss: 0.09360\n",
      "epochs [30/128] cost:15.00s train_loss: 0.61107 val_loss: 0.08459\n",
      "epochs [31/128] cost:15.00s train_loss: 0.63031 val_loss: 0.07360\n",
      "epochs [32/128] cost:15.00s train_loss: 0.63453 val_loss: 0.07433\n",
      "epochs [33/128] cost:15.00s train_loss: 0.64075 val_loss: 0.06931\n",
      "epochs [34/128] cost:15.00s train_loss: 0.59057 val_loss: 0.07570\n",
      "epochs [35/128] cost:15.00s train_loss: 0.62015 val_loss: 0.06988\n",
      "epochs [36/128] cost:15.00s train_loss: 0.58220 val_loss: 0.06501\n",
      "epochs [37/128] cost:15.00s train_loss: 0.57763 val_loss: 0.07928\n",
      "epochs [38/128] cost:15.00s train_loss: 0.61776 val_loss: 0.10157\n",
      "epochs [39/128] cost:15.00s train_loss: 0.61957 val_loss: 0.07078\n",
      "epochs [40/128] cost:15.00s train_loss: 0.59070 val_loss: 0.06387\n",
      "epochs [41/128] cost:15.00s train_loss: 0.57840 val_loss: 0.11533\n",
      "epochs [42/128] cost:15.00s train_loss: 0.61306 val_loss: 0.08103\n",
      "epochs [43/128] cost:15.00s train_loss: 0.60300 val_loss: 0.07000\n",
      "epochs [44/128] cost:15.00s train_loss: 0.56393 val_loss: 0.09190\n",
      "epochs [45/128] cost:15.00s train_loss: 0.59629 val_loss: 0.06783\n",
      "epochs [46/128] cost:15.00s train_loss: 0.60795 val_loss: 0.07258\n",
      "epochs [47/128] cost:15.00s train_loss: 0.57644 val_loss: 0.06375\n",
      "epochs [48/128] cost:15.00s train_loss: 0.56978 val_loss: 0.07598\n",
      "epochs [49/128] cost:15.00s train_loss: 0.58149 val_loss: 0.10728\n",
      "epochs [50/128] cost:15.00s train_loss: 0.61591 val_loss: 0.06472\n",
      "epochs [51/128] cost:15.00s train_loss: 0.57945 val_loss: 0.08818\n",
      "epochs [52/128] cost:15.00s train_loss: 0.58298 val_loss: 0.08503\n",
      "epochs [53/128] cost:15.00s train_loss: 0.55511 val_loss: 0.06235\n",
      "epochs [54/128] cost:15.00s train_loss: 0.53582 val_loss: 0.08055\n",
      "epochs [55/128] cost:15.00s train_loss: 0.58512 val_loss: 0.06182\n",
      "epochs [56/128] cost:15.00s train_loss: 0.56154 val_loss: 0.07816\n",
      "epochs [57/128] cost:15.00s train_loss: 0.57649 val_loss: 0.06718\n",
      "epochs [58/128] cost:15.00s train_loss: 0.57012 val_loss: 0.06954\n",
      "epochs [59/128] cost:15.00s train_loss: 0.56676 val_loss: 0.06580\n",
      "epochs [60/128] cost:15.00s train_loss: 0.55114 val_loss: 0.06295\n",
      "epochs [61/128] cost:15.00s train_loss: 0.54721 val_loss: 0.06119\n",
      "epochs [62/128] cost:15.00s train_loss: 0.54518 val_loss: 0.06240\n",
      "epochs [63/128] cost:15.00s train_loss: 0.54910 val_loss: 0.06369\n",
      "epochs [64/128] cost:15.00s train_loss: 0.53116 val_loss: 0.06539\n",
      "epochs [65/128] cost:15.00s train_loss: 0.53902 val_loss: 0.06008\n",
      "epochs [66/128] cost:15.00s train_loss: 0.55368 val_loss: 0.08694\n",
      "epochs [67/128] cost:15.00s train_loss: 0.54266 val_loss: 0.07419\n",
      "epochs [68/128] cost:15.00s train_loss: 0.52175 val_loss: 0.07070\n",
      "epochs [69/128] cost:15.00s train_loss: 0.50971 val_loss: 0.06030\n",
      "epochs [70/128] cost:15.00s train_loss: 0.54256 val_loss: 0.07397\n",
      "epochs [71/128] cost:15.00s train_loss: 0.52810 val_loss: 0.06351\n",
      "epochs [72/128] cost:15.00s train_loss: 0.52514 val_loss: 0.09220\n",
      "epochs [73/128] cost:15.00s train_loss: 0.58766 val_loss: 0.09010\n",
      "epochs [74/128] cost:15.00s train_loss: 0.57463 val_loss: 0.06966\n",
      "epochs [75/128] cost:15.00s train_loss: 0.52881 val_loss: 0.07604\n",
      "epochs [76/128] cost:15.00s train_loss: 0.55161 val_loss: 0.06707\n",
      "epochs [77/128] cost:15.00s train_loss: 0.52481 val_loss: 0.06812\n",
      "epochs [78/128] cost:15.00s train_loss: 0.51171 val_loss: 0.06508\n",
      "epochs [79/128] cost:15.00s train_loss: 0.52060 val_loss: 0.05748\n",
      "epochs [80/128] cost:15.00s train_loss: 0.52022 val_loss: 0.05933\n",
      "epochs [81/128] cost:15.00s train_loss: 0.50447 val_loss: 0.07192\n",
      "epochs [82/128] cost:15.00s train_loss: 0.51307 val_loss: 0.05935\n",
      "epochs [83/128] cost:15.00s train_loss: 0.50916 val_loss: 0.06834\n",
      "epochs [84/128] cost:15.00s train_loss: 0.51008 val_loss: 0.08574\n",
      "epochs [85/128] cost:15.00s train_loss: 0.54770 val_loss: 0.06584\n",
      "epochs [86/128] cost:15.00s train_loss: 0.50627 val_loss: 0.06340\n",
      "epochs [87/128] cost:15.00s train_loss: 0.53088 val_loss: 0.05938\n",
      "epochs [88/128] cost:15.00s train_loss: 0.50110 val_loss: 0.06048\n",
      "epochs [89/128] cost:15.00s train_loss: 0.53691 val_loss: 0.07196\n",
      "epochs [90/128] cost:15.00s train_loss: 0.54220 val_loss: 0.06630\n",
      "epochs [91/128] cost:15.00s train_loss: 0.50346 val_loss: 0.06022\n",
      "epochs [92/128] cost:15.00s train_loss: 0.51167 val_loss: 0.05672\n",
      "epochs [93/128] cost:15.00s train_loss: 0.52325 val_loss: 0.05855\n",
      "epochs [94/128] cost:15.00s train_loss: 0.54068 val_loss: 0.06869\n",
      "epochs [95/128] cost:15.00s train_loss: 0.50572 val_loss: 0.09607\n",
      "epochs [96/128] cost:15.00s train_loss: 0.52049 val_loss: 0.05768\n",
      "epochs [97/128] cost:15.00s train_loss: 0.48226 val_loss: 0.08741\n",
      "epochs [98/128] cost:15.00s train_loss: 0.50738 val_loss: 0.06236\n",
      "epochs [99/128] cost:15.00s train_loss: 0.47286 val_loss: 0.10074\n",
      "epochs [100/128] cost:15.00s train_loss: 0.52842 val_loss: 0.06549\n",
      "epochs [101/128] cost:15.00s train_loss: 0.48355 val_loss: 0.05756\n",
      "epochs [102/128] cost:15.00s train_loss: 0.46714 val_loss: 0.07327\n",
      "epochs [103/128] cost:15.00s train_loss: 0.47289 val_loss: 0.05929\n",
      "epochs [104/128] cost:15.00s train_loss: 0.48198 val_loss: 0.06518\n",
      "epochs [105/128] cost:15.00s train_loss: 0.47305 val_loss: 0.05635\n",
      "epochs [106/128] cost:15.00s train_loss: 0.48609 val_loss: 0.09867\n",
      "epochs [107/128] cost:15.00s train_loss: 0.51919 val_loss: 0.05109\n",
      "epochs [108/128] cost:15.00s train_loss: 0.45780 val_loss: 0.06162\n",
      "epochs [109/128] cost:15.00s train_loss: 0.44701 val_loss: 0.06451\n",
      "epochs [110/128] cost:15.00s train_loss: 0.46637 val_loss: 0.06067\n",
      "epochs [111/128] cost:15.00s train_loss: 0.49016 val_loss: 0.04910\n",
      "epochs [112/128] cost:15.00s train_loss: 0.45731 val_loss: 0.05465\n",
      "epochs [113/128] cost:15.00s train_loss: 0.44918 val_loss: 0.05090\n",
      "epochs [114/128] cost:15.00s train_loss: 0.46045 val_loss: 0.05120\n",
      "epochs [115/128] cost:15.00s train_loss: 0.46067 val_loss: 0.05783\n",
      "epochs [116/128] cost:15.00s train_loss: 0.45300 val_loss: 0.04894\n",
      "epochs [117/128] cost:15.00s train_loss: 0.44667 val_loss: 0.05113\n",
      "epochs [118/128] cost:15.00s train_loss: 0.43227 val_loss: 0.06861\n",
      "epochs [119/128] cost:15.00s train_loss: 0.47212 val_loss: 0.05058\n",
      "epochs [120/128] cost:15.00s train_loss: 0.43681 val_loss: 0.06428\n",
      "epochs [121/128] cost:15.00s train_loss: 0.41798 val_loss: 0.04773\n",
      "epochs [122/128] cost:15.00s train_loss: 0.44715 val_loss: 0.05142\n",
      "epochs [123/128] cost:15.00s train_loss: 0.43317 val_loss: 0.05797\n",
      "epochs [124/128] cost:15.00s train_loss: 0.43933 val_loss: 0.05113\n",
      "epochs [125/128] cost:15.00s train_loss: 0.42096 val_loss: 0.05221\n",
      "epochs [126/128] cost:15.00s train_loss: 0.43937 val_loss: 0.06729\n",
      "epochs [127/128] cost:15.00s train_loss: 0.41993 val_loss: 0.05647\n",
      "epochs [128/128] cost:15.00s train_loss: 0.42878 val_loss: 0.08010\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    starttime = datetime.datetime.now()\n",
    "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "    train_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    val_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    # training\n",
    "    total_train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "            X_wide_train_cuda = batch_x[0].float().cuda()\n",
    "            X_deep_train_cuda = batch_x[1].float().cuda()\n",
    "            y_train_cuda = batch_y.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
    "        loss = criterion(pred_y, y_train_cuda)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # validation\n",
    "    total_val_loss = 0\n",
    "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
    "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
    "            y_val_cuda = batch_val_y.cuda()\n",
    "        \n",
    "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
    "        val_loss = criterion(pred_y, y_val_cuda)\n",
    "        total_val_loss += val_loss.item()\n",
    "    \n",
    "        # print statistics\n",
    "    if min_val_loss > total_val_loss:\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        min_val_loss = total_val_loss\n",
    "    endtime = datetime.datetime.now()\n",
    "    print('epochs [%d/%d] cost:%.2fs train_loss: %.5f val_loss: %.5f' % \n",
    "          (epoch + 1, epochs, (endtime-starttime).seconds, total_train_loss, total_val_loss))\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = test[5].unique()\n",
    "test_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp = test[test[5]==year]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    test_list.append(temp)\n",
    "    \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2015\n",
      "len(tid) =  908\n",
      "len(pred_lat) = 908\n",
      "avg lat: 0.7807066341853878\n",
      "avg long: 0.8732786943208799\n",
      "avg distance error: 135.5011598761629\n",
      "Year: 2016\n",
      "len(tid) =  489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pred_lat) = 489\n",
      "avg lat: 0.8882787369024044\n",
      "avg long: 0.9464920824046984\n",
      "avg distance error: 150.52898812849236\n",
      "Year: 2017\n",
      "len(tid) =  544\n",
      "len(pred_lat) = 544\n",
      "avg lat: 0.8207940911545468\n",
      "avg long: 1.0388877419864428\n",
      "avg distance error: 149.61544467220156\n",
      "Year: 2018\n",
      "len(tid) =  806\n",
      "len(pred_lat) = 806\n",
      "avg lat: 0.8707500087416191\n",
      "avg long: 1.1385662883446173\n",
      "avg distance error: 161.55226884361366\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tid_list = []\n",
    "time_list = [] \n",
    "pred_lat_list = []\n",
    "pred_long_list = [] \n",
    "true_lat_list = [] \n",
    "true_long_list = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for year, _test in zip(years, test_list):\n",
    "\n",
    "        print('Year:', year)\n",
    "        # print(\"TID \", _test.loc[:,1])\n",
    "        y_test_lat = _test.loc[:,3]\n",
    "        \n",
    "        y_test_long = _test.loc[:,4]\n",
    "        \n",
    "        X_wide_test = X_wide_scaler.transform(_test.loc[:,6:])\n",
    "\n",
    "        final_test_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_test_dict[scaler_name][reanalysis_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_list.append(X_deep_temp)\n",
    "        X_deep_test = np.concatenate(final_test_list, axis=1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
    "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
    "\n",
    "        \n",
    "        tid  = _test.loc[:,1]\n",
    "        time_ = _test.loc[:,2]\n",
    "        print(\"len(tid) = \",len(tid))\n",
    "        pred = net(X_wide_test, X_deep_test)\n",
    "\n",
    "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy())\n",
    "\n",
    "        pred_lat = pred[:,0]\n",
    "        pred_long = pred[:,1]\n",
    "        \n",
    "        print(\"len(pred_lat) =\", len(pred_lat))\n",
    "        true_lat = y_test_lat\n",
    "        true_long = y_test_long\n",
    "\n",
    "        diff_lat = np.abs(pred_lat - true_lat)\n",
    "        diff_long = np.abs(pred_long - true_long)\n",
    "\n",
    "        print('avg lat:', sum(diff_lat)/len(diff_lat))\n",
    "        print('avg long:', sum(diff_long)/len(diff_long))\n",
    "\n",
    "        sum_error = []\n",
    "        for i in range(0, len(pred_lat)):\n",
    "            sum_error.append(great_circle((pred_lat[i], pred_long[i]), (true_lat[i], true_long[i])).kilometers)\n",
    "\n",
    "        print('avg distance error:', sum(sum_error)/len(sum_error))\n",
    "        \n",
    "        tid_list.append(tid)\n",
    "        time_list.append(time_)\n",
    "        pred_lat_list.append(pred_lat)\n",
    "        pred_long_list.append(pred_long)\n",
    "        true_lat_list.append(true_lat)\n",
    "        true_long_list.append(true_long)\n",
    "        \n",
    "\n",
    "tid_list_ =  [item for sublist in tid_list for item in sublist]\n",
    "time_list_ =  [item for sublist in time_list for item in sublist]\n",
    "pred_lat_list_ =  [item for sublist in pred_lat_list for item in sublist]\n",
    "pred_long_list_ =  [item for sublist in pred_long_list for item in sublist]\n",
    "true_lat_list_ =  [item for sublist in true_lat_list for item in sublist]\n",
    "true_long_list_ =  [item for sublist in true_long_list for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_key = pd.read_csv('../data/raw.csv', header=None)\n",
    "track_data = [] \n",
    "\n",
    "for i in range(len(tid_list_)):\n",
    "\n",
    "    if len(id_key[id_key[0] == str(tid_list_[i])][11].unique()) == 0:\n",
    "\n",
    "        track_data.append([\n",
    "            tid_list_[i], \n",
    "            id_key[id_key[0] == tid_list_[i]][11].unique()[0],\n",
    "            time_list_[i],\n",
    "            true_lat_list_[i],\n",
    "            true_long_list_[i],\n",
    "            pred_lat_list_[i],\n",
    "            pred_long_list_[i]\n",
    "            ])\n",
    "        \n",
    "    else:\n",
    "\n",
    "        track_data.append([\n",
    "            tid_list_[i], \n",
    "            id_key[id_key[0] == str(tid_list_[i])][11].unique()[0],\n",
    "            time_list_[i],\n",
    "            true_lat_list_[i],\n",
    "            true_long_list_[i],\n",
    "            pred_lat_list_[i],\n",
    "            pred_long_list_[i]\n",
    "            ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been written to ./QPA_track_data/track_data_classical_pruning_01.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = \"../results/QPA_track_data/track_data_classical_pruning_01.csv\"\n",
    "# Define the column headers\n",
    "headers = [\"TID\", \"KEY\", \"TIME\", \"LAT\", \"LONG\", \"PRED_LAT\", \"PRED_LONG\"] \n",
    "\n",
    "# Write to CSV\n",
    "with open(file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # Write the data rows\n",
    "    writer.writerows(track_data)\n",
    "\n",
    "print(f\"CSV file has been written to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
